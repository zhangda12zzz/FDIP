import os
import sys
import time
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import seaborn as sns     # åŸºäºmatplotlibçš„é«˜çº§å¯è§†åŒ–åº“ï¼Œç”¨äºåˆ›å»ºç»Ÿè®¡å›¾è¡¨
import matplotlib.pyplot as plt   # ç»˜å›¾åº“
from torch.cuda.amp import autocast, GradScaler     #float16
from torch.utils.data import DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter          # è®­ç»ƒæŒ‡æ ‡å†™å…¥TensorBoardæ—¥å¿—
from tqdm import tqdm
import json
import pickle    # åºåˆ—åŒ–
import pandas as pd   # è¡¨æ ¼æ•°æ®åˆ†æ
from datetime import datetime
from articulate.math import r6d_to_rotation_matrix
from data.dataset_posReg import ImuDataset
from model.net_zd import FDIP_1, FDIP_2, FDIP_3
from evaluator import PoseEvaluator, PerFramePoseEvaluator   # æ•´ä½“ã€é€å¸§å§¿æ€è¯„ä¼°
import gc
import argparse

# --- Configuration ---
os.environ["CUDA_VISIBLE_DEVICES"] = '0'  # è®¾ç½®ä½¿ç”¨çš„GPU
LEARNING_RATE = 1e-4
WEIGHT_DECAY = 1e-3                       # æ­£åˆ™åŒ–å‚æ•°ï¼Œæ§åˆ¶æƒé‡è¡°å‡
BATCH_SIZE = 64
DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
LOG_ENABLED = True
TRAIN_PERCENT = 0.9
BATCH_SIZE_VAL = 32
SEED = 42

PATIENCE = 20
MAX_EPOCHS = 150
DELTA = 0

# --- Paths ---
# è¯·ç¡®ä¿è¿™äº›è·¯å¾„åœ¨æ‚¨çš„ç¯å¢ƒä¸­æ˜¯æ­£ç¡®çš„
TRAIN_DATA_FOLDERS = [
    os.path.join("D:\\", "Dataset", "TotalCapture_Real_60FPS", "KaPt", "split_actions"),
    os.path.join("D:\\", "Dataset", "DIPIMUandOthers", "DIP_6", "Detail"),
    os.path.join("D:\\", "Dataset", "AMASS", "DanceDB", "pt"),
    os.path.join("D:\\", "Dataset", "AMASS", "HumanEva", "pt"),

]
VAL_DATA_FOLDERS = [
    os.path.join("D:\\", "Dataset", "SingleOne",  "pt"),
]

TIMESTAMP = None
CHECKPOINT_DIR = None
LOG_DIR = "log"
LOG_RUN_DIR = None


class DataNormalizer:
    """æ•°æ®æ ‡å‡†åŒ–å™¨ï¼Œç¡®ä¿è®­ç»ƒé›†å’ŒéªŒè¯é›†ä½¿ç”¨ç›¸åŒçš„å½’ä¸€åŒ–å‚æ•°"""
    def __init__(self):
        self.stats = {}
        self.fitted = False

    def fit(self, data_loader, device=DEVICE):
        """åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆç»Ÿè®¡é‡"""
        print("Computing normalization statistics from training data...")

        # åˆå§‹åŒ–ç´¯ç§¯å˜é‡
        acc_sum = torch.zeros(6, 3, device=device)  # 6ä¸ªä¼ æ„Ÿå™¨ï¼Œ3è½´åŠ é€Ÿåº¦
        ori_sum = torch.zeros(6, 6, device=device)  # 6ä¸ªä¼ æ„Ÿå™¨ï¼Œ6Dæ–¹å‘
        acc_sq_sum = torch.zeros(6, 3, device=device)
        ori_sq_sum = torch.zeros(6, 6, device=device)

        total_samples = 0

        with torch.no_grad():
            for batch_idx, data in enumerate(tqdm(data_loader, desc="Computing stats")):
                acc = data[0].to(device, non_blocking=True).float()  # [B, S, 6, 3]
                ori_6d = data[2].to(device, non_blocking=True).float()  # [B, S, 6, 6]

                batch_size, seq_len = acc.shape[:2]

                # é‡å¡‘ä¸º [B*S, 6, 3] å’Œ [B*S, 6, 6]
                acc_flat = acc.view(-1, 6, 3)
                ori_flat = ori_6d.view(-1, 6, 6)

                # ç´¯ç§¯ç»Ÿè®¡é‡
                acc_sum += acc_flat.sum(dim=0)
                ori_sum += ori_flat.sum(dim=0)
                acc_sq_sum += (acc_flat ** 2).sum(dim=0)
                ori_sq_sum += (ori_flat ** 2).sum(dim=0)

                total_samples += batch_size * seq_len

                # ä¸ºäº†èŠ‚çœå†…å­˜ï¼Œåªä½¿ç”¨éƒ¨åˆ†æ•°æ®è®¡ç®—ç»Ÿè®¡é‡
                if batch_idx >= 100:  # ä½¿ç”¨å‰100ä¸ªbatchè®¡ç®—ç»Ÿè®¡é‡
                    break

        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        self.stats['acc_mean'] = acc_sum / total_samples
        self.stats['ori_mean'] = ori_sum / total_samples
        self.stats['acc_std'] = torch.sqrt(acc_sq_sum / total_samples - self.stats['acc_mean'] ** 2)
        self.stats['ori_std'] = torch.sqrt(ori_sq_sum / total_samples - self.stats['ori_mean'] ** 2)

        # é˜²æ­¢æ ‡å‡†å·®ä¸º0
        self.stats['acc_std'] = torch.clamp(self.stats['acc_std'], min=1e-6)
        self.stats['ori_std'] = torch.clamp(self.stats['ori_std'], min=1e-6)

        self.fitted = True

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("Normalization statistics computed:")
        print(f"  Acc mean: {self.stats['acc_mean'].mean().item():.6f}")
        print(f"  Acc std: {self.stats['acc_std'].mean().item():.6f}")
        print(f"  Ori mean: {self.stats['ori_mean'].mean().item():.6f}")
        print(f"  Ori std: {self.stats['ori_std'].mean().item():.6f}")

    def transform_batch(self, acc, ori_6d):
        """å¯¹å•ä¸ªbatchè¿›è¡Œæ ‡å‡†åŒ–"""
        if not self.fitted:
            raise ValueError("Normalizer must be fitted before transform")

        # æ ‡å‡†åŒ–
        acc_norm = (acc - self.stats['acc_mean'].unsqueeze(0).unsqueeze(0)) / self.stats['acc_std'].unsqueeze(
            0).unsqueeze(0)
        ori_norm = (ori_6d - self.stats['ori_mean'].unsqueeze(0).unsqueeze(0)) / self.stats['ori_std'].unsqueeze(
            0).unsqueeze(0)

        return acc_norm, ori_norm

    def save_stats(self, path):
        """ä¿å­˜å½’ä¸€åŒ–ç»Ÿè®¡é‡"""
        if self.fitted:
            torch.save(self.stats, path)
            print(f"Normalization stats saved to: {path}")

    def load_stats(self, path):
        """åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡é‡"""
        self.stats = torch.load(path)
        self.fitted = True
        print(f"Normalization stats loaded from: {path}")


def create_directories():
    """åˆ›å»ºå¿…è¦çš„ç›®å½•ï¼Œlogå†…éƒ¨å¸¦æ—¶é—´æˆ³å­æ–‡ä»¶å¤¹ã€‚"""
    dirs = [
        os.path.join(CHECKPOINT_DIR, "ggip1"),
        os.path.join(CHECKPOINT_DIR, "ggip2"),
        os.path.join(CHECKPOINT_DIR, "ggip3"),
        LOG_DIR,
        LOG_RUN_DIR,
    ]
    for dir_path in dirs:
        os.makedirs(dir_path, exist_ok=True)
    print(f"Directories created with timestamp {TIMESTAMP}:")
    for dir_path in dirs:
        print(f"  - {dir_path}")

def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)  # é€‚ç”¨äºå¤šGPU
        # ç¡®ä¿CUDAå·ç§¯æ“ä½œçš„ç¡®å®šæ€§
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False  # Trueå¯èƒ½åŠ é€Ÿä½†å¼•å…¥ä¸ç¡®å®šæ€§
    print(f"Random seed set to {seed}")


def setup_directories_and_paths(args):
    """æ ¹æ®å‘½ä»¤è¡Œå‚æ•°è®¾ç½®å…¨å±€è·¯å¾„å˜é‡"""
    global TIMESTAMP, CHECKPOINT_DIR, LOG_RUN_DIR

    # ç¡®å®šæ—¶é—´æˆ³å’Œæ£€æŸ¥ç‚¹ç›®å½•
    if args.resume:
        TIMESTAMP = args.resume
        CHECKPOINT_DIR = os.path.join("GGIP", f"checkpoints_{TIMESTAMP}")
        print(f"Resuming training from timestamp: {TIMESTAMP}")
    elif args.checkpoint_dir:
        CHECKPOINT_DIR = args.checkpoint_dir
        TIMESTAMP = os.path.basename(CHECKPOINT_DIR).replace("checkpoints_", "")
        print(f"Using checkpoint directory: {CHECKPOINT_DIR}")
    else:
        TIMESTAMP = datetime.now().strftime("%Y%m%d_%H%M%S")
        CHECKPOINT_DIR = os.path.join("GGIP", f"checkpoints_{TIMESTAMP}")
        print(f"Starting new training with timestamp: {TIMESTAMP}")

    # è®¾ç½®æ—¥å¿—ç›®å½•
    LOG_RUN_DIR = os.path.join(LOG_DIR, TIMESTAMP)

    # éªŒè¯æ£€æŸ¥ç‚¹ç›®å½•æ˜¯å¦å­˜åœ¨ï¼ˆä»…å¯¹ç»­è®­æƒ…å†µï¼‰
    if not os.path.exists(CHECKPOINT_DIR) and (args.resume or args.checkpoint_dir):
        print(f"Error: Checkpoint directory {CHECKPOINT_DIR} does not exist!")
        sys.exit(1)

    print(f"Global paths set:")
    print(f"  - TIMESTAMP: {TIMESTAMP}")
    print(f"  - CHECKPOINT_DIR: {CHECKPOINT_DIR}")
    print(f"  - LOG_RUN_DIR: {LOG_RUN_DIR}")

def clear_memory():
    """æ¸…ç†GPUå’ŒCPUå†…å­˜"""
    torch.cuda.empty_cache()
    gc.collect()
    print(f"GPU memory after cleanup: {torch.cuda.memory_allocated()/1024**3:.2f} GB")

def cleanup_training_objects(*objects):
    """æ¸…ç†è®­ç»ƒç›¸å…³å¯¹è±¡"""
    for obj in objects:
        if obj is not None:
            del obj
    clear_memory()


class EarlyStopping:
    """
    å¦‚æœéªŒè¯æŸå¤±åœ¨ç»™å®šçš„è€å¿ƒæœŸåæ²¡æœ‰æ”¹å–„ï¼Œåˆ™æå‰åœæ­¢è®­ç»ƒï¼ˆä»ä¼šè¿›è¡Œä¸‹ä¸€é˜¶æ®µè®­ç»ƒï¼‰ã€‚
    MODIFIED: ç°åœ¨å¯ä»¥ä¿å­˜å’ŒåŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€å’Œè½®æ•°ã€‚
    """

    def __init__(self, patience=10, verbose=True, delta=0, path='checkpoint.pt'):
        self.patience = patience    # å…è®¸éªŒè¯æŸå¤±ä¸æ”¹å–„çš„è½®æ•°ä¸Šé™
        self.verbose = verbose      # æ§åˆ¶æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        self.counter = 0            # ç”¨äºè®°å½•è¿ç»­æ²¡æœ‰æ”¹å–„çš„è½®æ•°
        self.best_score = None      # è®°å½•æœ€ä½³åˆ†æ•°ï¼ˆ-val_lossï¼‰ã€æœ€ä½³=æœ€å¤§ã€‘
        self.early_stop = False
        self.val_loss_min = np.Inf  # è®°å½•æœ€ä½éªŒè¯æŸå¤±
        self.delta = delta          # å®šä¹‰"æ˜¾è‘—æ”¹å–„"çš„æœ€å°é˜ˆå€¼
        self.path = path
        self.best_epoch = 0         # è®°å½•è¾¾åˆ°æœ€ä½éªŒè¯æŸå¤±çš„è½®æ•°

    def __call__(self, val_loss, model, optimizer, epoch):

        if not np.isfinite(val_loss):
            if self.verbose:
                print(f"Warning: Validation loss is {val_loss} at epoch {epoch}, skipping EarlyStopping.")
            return

        score = -val_loss            # å°†æŸå¤±è½¬æ¢ä¸ºåˆ†æ•°ï¼Œè¶Šå¤§è¶Šå¥½
        if self.best_score is None:  # ç¬¬ä¸€æ¬¡è°ƒç”¨
            self.best_score = score
            self.save_checkpoint(val_loss, model, optimizer, epoch)
        elif score < self.best_score + self.delta:  # åˆ†æ•°æ²¡æœ‰æ”¹å–„
            self.counter += 1
            if self.verbose:                        # æ‰“å°ä¿¡æ¯
                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:       # è¾¾åˆ°è€å¿ƒä¸Šé™
                self.early_stop = True
        else:                                       # åˆ†æ•°æœ‰æ‰€æ”¹å–„
            self.best_score = score
            self.save_checkpoint(val_loss, model, optimizer, epoch)
            self.counter = 0                        # é‡ç½®è®¡æ•°å™¨

    def save_checkpoint(self, val_loss, model, optimizer, epoch):
        """å½“éªŒè¯æŸå¤±å‡å°‘æ—¶ä¿å­˜æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œè½®æ•°ã€‚"""
        if self.verbose:
            print(
                f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving checkpoint to {self.path}...')

        os.makedirs(os.path.dirname(self.path), exist_ok=True)

        checkpoint = {
            'epoch': epoch,                                   # å½“å‰è½®æ•°
            'model_state_dict': model.state_dict(),           # æ¨¡å‹å‚æ•°
            'optimizer_state_dict': optimizer.state_dict(),   # ä¼˜åŒ–å™¨çŠ¶æ€
            'val_loss_min': val_loss,                         # å½“å‰æœ€ä½³éªŒè¯æŸå¤±
            'best_score': self.best_score,                    # å½“å‰æœ€ä½³åˆ†æ•°
            'early_stopping_counter': self.counter            # æ—©åœè®¡æ•°å™¨çŠ¶æ€
        }
        torch.save(checkpoint, self.path)
        self.val_loss_min = val_loss
        self.best_epoch = epoch


def load_data_unified_split(train_percent=0.8, val_percent=0.2, seed=None):
    """
    ç»Ÿä¸€åŠ è½½æ‰€æœ‰æ•°æ®é›†ï¼Œç„¶åéšæœºåˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
    è¿™æ ·å¯ä»¥ç¡®ä¿è®­ç»ƒé›†å’ŒéªŒè¯é›†æ¥è‡ªç›¸åŒçš„æ•°æ®åˆ†å¸ƒ

    å‚æ•°:
        train_percent: è®­ç»ƒé›†æ¯”ä¾‹
        val_percent: éªŒè¯é›†æ¯”ä¾‹
        seed: éšæœºç§å­ï¼Œç”¨äºç¡®ä¿å¯é‡ç°çš„åˆ’åˆ†
    """
    print("Loading unified dataset with consistent split method...")

    if seed is not None:
        torch.manual_seed(seed)
        np.random.seed(seed)

    try:
        all_data_folders = TRAIN_DATA_FOLDERS + VAL_DATA_FOLDERS
        print(f"Combining datasets from {len(all_data_folders)} folders:")
        for folder in all_data_folders:
            print(f"  - {folder}")

        unified_dataset = ImuDataset(all_data_folders)
        total_size = len(unified_dataset)
        print(f"Total unified dataset size: {total_size} samples")

        train_size = int(total_size * train_percent)
        val_size = total_size - train_size  # å‰©ä½™çš„éƒ½ç»™éªŒè¯é›†

        print(f"Dataset split:")
        print(f"  - Training: {train_size} samples ({train_size / total_size * 100:.1f}%)")
        print(f"  - Validation: {val_size} samples ({val_size / total_size * 100:.1f}%)")

        from torch.utils.data import random_split

        train_dataset, val_dataset = random_split(
            unified_dataset,
            [train_size, val_size],
            generator=torch.Generator().manual_seed(seed) if seed else None
        )

    except Exception as e:
        print(f"Error loading unified dataset: {e}")
        sys.exit(1)

    num_workers = 0 if sys.platform == "win32" else 4

    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        pin_memory=True,
        num_workers=num_workers
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE_VAL,
        shuffle=False,
        pin_memory=True,
        num_workers=num_workers
    )

    print(f"Data loaders created successfully!")
    print(f"  - Training batches: {len(train_loader)}")
    print(f"  - Validation batches: {len(val_loader)}")

    return train_loader, val_loader


def check_data_distribution(train_loader, val_loader, num_samples=5):
    """
    æ£€æŸ¥è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ•°æ®åˆ†å¸ƒä¸€è‡´æ€§

    å‚æ•°:
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        num_samples: ç”¨äºç»Ÿè®¡çš„æ ·æœ¬æ‰¹æ¬¡æ•°
    """
    print("\n=== Data Distribution Analysis ===")

    def compute_stats(data_loader, name, max_batches=num_samples):
        """è®¡ç®—æ•°æ®é›†çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'acc_mean': [],
            'acc_std': [],
            'ori_mean': [],
            'ori_std': [],
            'pos_mean': [],
            'pos_std': []
        }

        count = 0
        with torch.no_grad():
            for data in data_loader:
                if count >= max_batches:
                    break

                acc = data[0].float()
                ori = data[2].float()
                pos = data[3].float()

                stats['acc_mean'].append(acc.mean().item())
                stats['acc_std'].append(acc.std().item())
                stats['ori_mean'].append(ori.mean().item())
                stats['ori_std'].append(ori.std().item())
                stats['pos_mean'].append(pos.mean().item())
                stats['pos_std'].append(pos.std().item())

                count += 1

        # è®¡ç®—å¹³å‡å€¼
        for key in stats:
            stats[key] = np.mean(stats[key])

        return stats

    # è®¡ç®—è®­ç»ƒé›†å’ŒéªŒè¯é›†ç»Ÿè®¡
    print("Computing training set statistics...")
    train_stats = compute_stats(train_loader, "Train")

    print("Computing validation set statistics...")
    val_stats = compute_stats(val_loader, "Validation")

    # æ‰“å°å¯¹æ¯”ç»“æœ
    print(f"\nDistribution Comparison (based on {num_samples} batches):")
    print(f"{'Metric':<15} {'Train':<12} {'Validation':<12} {'Difference':<12}")
    print("-" * 55)

    for key in train_stats:
        train_val = train_stats[key]
        val_val = val_stats[key]
        diff = abs(train_val - val_val)
        print(f"{key:<15} {train_val:<12.6f} {val_val:<12.6f} {diff:<12.6f}")

    # è®¡ç®—æ€»ä½“ç›¸ä¼¼åº¦å¾—åˆ†
    total_diff = sum([abs(train_stats[key] - val_stats[key]) for key in train_stats])
    print(f"\nTotal Difference Score: {total_diff:.6f} (lower is better)")

    if total_diff < 0.1:
        print("âœ“ Data distributions appear consistent!")
    elif total_diff < 0.5:
        print("âš  Data distributions have minor differences")
    else:
        print("âŒ Data distributions have significant differences")

    return train_stats, val_stats


def load_data_separate():
    """
    åˆ†åˆ«åŠ è½½è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œé¿å…æ•°æ®æ³„æ¼
    """
    print("Loading separate train and validation datasets...")

    try:
        # åŠ è½½è®­ç»ƒæ•°æ®é›†
        print("Loading training dataset...")
        train_dataset = ImuDataset(TRAIN_DATA_FOLDERS)
        print(f"Training dataset loaded: {len(train_dataset)} samples")

        # åŠ è½½éªŒè¯æ•°æ®é›†
        print("Loading validation dataset...")
        val_dataset = ImuDataset(VAL_DATA_FOLDERS)
        print(f"Validation dataset loaded: {len(val_dataset)} samples")

    except Exception as e:
        print(f"Error loading datasets: {e}")
        print("Please ensure your dataset paths and ImuDataset class are correct.")
        sys.exit(1)

    # æ•°æ®åŠ è½½å™¨è®¾ç½®
    num_workers = 0 if sys.platform == "win32" else 4

    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        pin_memory=True,
        num_workers=num_workers
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE_VAL,
        shuffle=False,
        pin_memory=True,
        num_workers=num_workers
    )

    print(f"Data loaders created successfully!")
    print(f"  - Training batches: {len(train_loader)}")
    print(f"  - Validation batches: {len(val_loader)}")

    return train_loader, val_loader


def load_data_legacy(train_percent=0.9):
    """
    åŸå§‹çš„æ•°æ®åŠ è½½æ–¹å¼ï¼ˆä½œä¸ºå¤‡é€‰æ–¹æ¡ˆï¼‰
    ä»åŒä¸€æ•°æ®é›†ä¸­æŒ‰æ¯”ä¾‹åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯é›†
    """
    print("Loading dataset with legacy split method...")
    try:
        custom_dataset = ImuDataset(TRAIN_DATA_FOLDERS)
    except Exception as e:
        print(f"Error loading dataset: {e}")
        sys.exit(1)

    total_size = len(custom_dataset)
    train_size = int(total_size * train_percent)
    train_indices = list(range(train_size))
    val_indices = list(range(train_size, total_size))

    train_dataset = Subset(custom_dataset, train_indices)
    val_dataset = Subset(custom_dataset, val_indices)

    num_workers = 0 if sys.platform == "win32" else 4

    train_loader = DataLoader(
        train_dataset, batch_size=BATCH_SIZE, shuffle=True,
        pin_memory=True, num_workers=num_workers
    )
    val_loader = DataLoader(
        val_dataset, batch_size=BATCH_SIZE_VAL, shuffle=False,
        pin_memory=True, num_workers=num_workers
    )
    print(f"Legacy dataset loaded: {len(train_dataset)} training samples, {len(val_dataset)} validation samples")
    return train_loader, val_loader

def train_fdip_1(model, optimizer, scheduler, train_loader, val_loader, epochs, early_stopper, start_epoch=0):
    """è®­ç»ƒ FDIP_1 æ¨¡å‹ï¼Œæ”¯æŒä»æŒ‡å®šè½®æ•°å¼€å§‹è®­ç»ƒ"""
    print("\n=============================== Starting FDIP_1 Training =============================")
    criterion = nn.MSELoss()
    scaler = GradScaler()
    # åˆ›å»ºSummaryWriterå®ä¾‹
    writer = SummaryWriter(os.path.join(LOG_RUN_DIR, 'ggip1')) if LOG_ENABLED else None

    # ä» start_epoch å¼€å§‹å¾ªç¯ï¼Œend_epoch ä¸º epochs - 1
    for epoch in range(start_epoch, epochs):
        current_epoch = epoch + 1  # ç”¨äºæ—¥å¿—æ˜¾ç¤ºï¼Œä»1å¼€å§‹
        model.train()
        train_losses = []
        epoch_pbar = tqdm(train_loader, desc=f"FDIP_1 Epoch {current_epoch}/{epochs}", leave=True)
        # æ¯ä¸ªæ‰¹æ¬¡ out_acc, out_ori, out_rot_6d, out_leaf_pos, out_all_pos,  out_pose, out_pose_6d, out_shape
        for data in epoch_pbar:
            acc = data[0].to(DEVICE, non_blocking=True).float()
            ori_6d = data[2].to(DEVICE, non_blocking=True).float()
            p_leaf = data[3].to(DEVICE, non_blocking=True).float()

            x = torch.cat((acc, ori_6d), -1).view(acc.shape[0], acc.shape[1], -1)
            target = p_leaf.view(-1, p_leaf.shape[1], 15)                       # 5ä¸ªå¶èŠ‚ç‚¹ï¼Œæ¯ä¸ª3Dä½ç½®

            optimizer.zero_grad(set_to_none=True)
            with autocast():
                logits = model(x)
                loss = torch.sqrt(criterion(logits, target))                    # ä½¿ç”¨RMSE

            if torch.isnan(loss) or torch.isinf(loss):
                print(f"Warning: NaN/Inf loss encountered at FDIP_1 Epoch {current_epoch}, skipping batch.")
                continue

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            train_losses.append(loss.item())
            epoch_pbar.set_postfix(loss=f"{loss.item():.4f}")

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_losses = []
        with torch.no_grad():
            for data_val in val_loader:
                acc_val = data_val[0].to(DEVICE, non_blocking=True).float()
                ori_val = data_val[2].to(DEVICE, non_blocking=True).float()
                p_leaf_val = data_val[3].to(DEVICE, non_blocking=True).float()

                x_val = torch.cat((acc_val, ori_val), -1).view(acc_val.shape[0], acc_val.shape[1], -1)
                target_val = p_leaf_val.view(-1, p_leaf_val.shape[1], 15)
                logits_val = model(x_val)
                loss_val = torch.sqrt(criterion(logits_val, target_val))

                if torch.isnan(loss_val) or torch.isinf(loss_val):
                    print(
                        f"Warning: NaN/Inf loss encountered at FDIP_1 Epoch {current_epoch}, skipping validation batch.")
                    continue

                val_losses.append(loss_val.item())

        avg_train_loss = np.mean(train_losses) if train_losses else 0.0      # å¤„ç†ç©ºåˆ—è¡¨æƒ…å†µ
        avg_val_loss = np.mean(val_losses) if val_losses else 0.0            # å¤„ç†ç©ºåˆ—è¡¨æƒ…å†µ
        current_lr = optimizer.param_groups[0]['lr']
        print(
            f'FDIP_1 Epoch {current_epoch}/{epochs} | Avg Train Loss: {avg_train_loss:.6f} | Avg Val Loss: {avg_val_loss:.6f} | LR: {current_lr:.6f}')

        if LOG_ENABLED and writer:
            writer.add_scalars('loss/fdip1', {'train': avg_train_loss, 'val': avg_val_loss}, current_epoch)
            writer.add_scalar('learning_rate/fdip1', current_lr, current_epoch)

        scheduler.step()                                                     # å­¦ä¹ ç‡è°ƒåº¦å™¨æ­¥è¿›

        # æ£€æŸ¥æ—©åœ
        early_stopper(avg_val_loss, model, optimizer, current_epoch)
        if early_stopper.early_stop:
            print(f"Early stopping triggered at epoch {current_epoch} for FDIP_1.")
            break

        # ğŸ”¥ æ¯ä¸ªepochç»“æŸåæ¸…ç†å†…å­˜
        torch.cuda.empty_cache()

    # è®­ç»ƒç»“æŸåï¼ŒåŠ è½½æœ€ä½³æ¨¡å‹çš„çŠ¶æ€
    print(
        f"FDIP_1 training finished. Loading best model from epoch {early_stopper.best_epoch} saved at {early_stopper.path}.")
    if os.path.exists(early_stopper.path):
        best_checkpoint = torch.load(early_stopper.path)
        model.load_state_dict(best_checkpoint['model_state_dict'])
    else:
        print(f"Warning: Best model checkpoint not found at {early_stopper.path}. Using last epoch's model.")

    if writer:
        writer.close()
        del writer  # ğŸ”¥ æ¸…ç†writer

    # ğŸ”¥ æ¸…ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸´æ—¶å˜é‡
    del criterion, scaler
    torch.cuda.empty_cache()

    print("======================== FDIP_1 Training Finished ==========================================")
    return model


def train_fdip_2(model1, model2, optimizer, scheduler, train_loader, val_loader, epochs, early_stopper, start_epoch=0):
    """è®­ç»ƒ FDIP_2 æ¨¡å‹ï¼Œæ”¯æŒä»æŒ‡å®šè½®æ•°å¼€å§‹è®­ç»ƒ"""
    print("\n====================== Starting FDIP_2 Training (Online Inference) =========================")
    criterion = nn.MSELoss()
    scaler = GradScaler()
    writer = SummaryWriter(os.path.join(LOG_RUN_DIR, 'ggip2')) if LOG_ENABLED else None

    for epoch in range(start_epoch, epochs):
        current_epoch = epoch + 1
        model1.eval()  # model1åœ¨FDIP_2è®­ç»ƒæ—¶ä¸è¿›è¡Œè®­ç»ƒï¼Œåªåšå‰å‘æ¨æ–­
        model2.train()
        train_losses = []
        epoch_pbar = tqdm(train_loader, desc=f"FDIP_2 Epoch {current_epoch}/{epochs}", leave=True)
        for data in epoch_pbar:
            acc = data[0].to(DEVICE, non_blocking=True).float()
            ori_6d = data[2].to(DEVICE, non_blocking=True).float()
            p_all = data[4].to(DEVICE, non_blocking=True).float()  # 24ä¸ªå…³èŠ‚çš„3Dä½ç½®

            with torch.no_grad():  # FDIP_1 çš„æ¨æ–­ä¸è®¡ç®—æ¢¯åº¦
                input1 = torch.cat((acc, ori_6d), -1).view(acc.shape[0], acc.shape[1], -1)
                p_leaf_logits = model1(input1)
                # p_leaf_logitsæ˜¯5ä¸ªå¶èŠ‚ç‚¹çš„é¢„æµ‹ä½ç½®ï¼Œéœ€è¦æ‹¼æ¥æ ¹èŠ‚ç‚¹ï¼ˆç¬¬0ä¸ªèŠ‚ç‚¹ï¼‰çš„0ä½ç½®
                # å½¢çŠ¶: [B, S, 5*3] -> [B, S, 6, 3]
                zeros = torch.zeros(p_leaf_logits.shape[0], p_leaf_logits.shape[1], 3, device=DEVICE)  # æ ¹èŠ‚ç‚¹çš„3Dä½ç½®æ˜¯0
                p_leaf_pred = torch.cat(
                    [zeros, p_leaf_logits.view(p_leaf_logits.shape[0], p_leaf_logits.shape[1], -1)], dim=2)

            # FDIP_2 çš„è¾“å…¥æ˜¯acc, ori_6då’Œp_leaf_pred
            # è¿™é‡Œçš„p_leaf_predå½¢çŠ¶æ˜¯[B, S, 6, 3]ï¼Œä¸accå’Œori_6dçš„èŠ‚ç‚¹ç»´åº¦å¯¹é½
            # æ‹¼æ¥æ—¶éœ€è¦å±•å¹³
            x2 = torch.cat([acc, ori_6d, p_leaf_pred.view(p_leaf_pred.shape[0], p_leaf_pred.shape[1], 6, 3)],dim=-1).view(
                acc.shape[0], acc.shape[1], -1)

            # targetæ˜¯æ‰€æœ‰24ä¸ªå…³èŠ‚çš„3Dä½ç½®ï¼Œæ ¹å…³èŠ‚ä½ç½®è¡¥0 (è¿™æ˜¯é’ˆå¯¹æ¨¡å‹è¾“å‡ºè®¾è®¡çš„ï¼Œæ ¹å…³èŠ‚ä½ç½®ä¸º0)
            target = torch.cat([torch.zeros_like(p_all[:, :, 0:1, :]), p_all], dim=2).view(p_all.shape[0],
                                                                                           p_all.shape[1], -1)  # 24*3

            optimizer.zero_grad(set_to_none=True)
            with autocast():
                logits = model2(x2)
                loss = torch.sqrt(criterion(logits, target))

            if torch.isnan(loss) or torch.isinf(loss):
                print(f"Warning: NaN/Inf loss encountered at FDIP_2 Epoch {current_epoch}, skipping batch.")
                continue

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            train_losses.append(loss.item())
            epoch_pbar.set_postfix(loss=f"{loss.item():.4f}")

        # éªŒè¯é˜¶æ®µ
        model2.eval()
        val_losses = []
        with torch.no_grad():
            for data_val in val_loader:
                acc_val, ori_val, p_all_val = [d.to(DEVICE, non_blocking=True).float() for d in
                                               (data_val[0], data_val[2], data_val[4])]
                input1_val = torch.cat((acc_val, ori_val), -1).view(acc_val.shape[0], acc_val.shape[1], -1)
                p_leaf_logits_val = model1(input1_val)
                zeros_val = torch.zeros(p_leaf_logits_val.shape[0], p_leaf_logits_val.shape[1],1, 3, device=DEVICE)
                p_leaf_pred_val = torch.cat(
                    [zeros_val, p_leaf_logits_val.view(p_leaf_logits_val.shape[0], p_leaf_logits_val.shape[1], 5, 3)],
                    dim=2)

                x2_val = torch.cat(
                    (acc_val, ori_val, p_leaf_pred_val),
                    -1).view(acc_val.shape[0], acc_val.shape[1], -1)
                target_val = torch.cat([torch.zeros_like(p_all_val[:, :, 0:1, :]), p_all_val], dim=2).view(
                    p_all_val.shape[0],
                    p_all_val.shape[1], -1)
                logits_val = model2(x2_val)
                loss_val = torch.sqrt(criterion(logits_val, target_val))

                if torch.isnan(loss_val) or torch.isinf(loss_val):
                    print(
                        f"Warning: NaN/Inf loss encountered at FDIP_2 Epoch {current_epoch}, skipping validation batch.")
                    continue

                val_losses.append(loss_val.item())

        avg_train_loss = np.mean(train_losses) if train_losses else 0.0
        avg_val_loss = np.mean(val_losses) if val_losses else 0.0
        current_lr = optimizer.param_groups[0]['lr']
        print(
            f'FDIP_2 Epoch {current_epoch}/{epochs} | Avg Train Loss: {avg_train_loss:.6f} | Avg Val Loss: {avg_val_loss:.6f} | LR: {current_lr:.6f}')

        if LOG_ENABLED and writer:
            writer.add_scalars('loss/fdip2', {'train': avg_train_loss, 'val': avg_val_loss}, current_epoch)
            writer.add_scalar('learning_rate/fdip2', current_lr, current_epoch)

        scheduler.step()
        early_stopper(avg_val_loss, model2, optimizer, current_epoch)
        if early_stopper.early_stop:
            print(f"Early stopping triggered at epoch {current_epoch} for FDIP_2.")
            break

        # ğŸ”¥ æ¯ä¸ªepochç»“æŸåæ¸…ç†å†…å­˜
        torch.cuda.empty_cache()

    print(
        f"FDIP_2 training finished. Loading best model from epoch {early_stopper.best_epoch} saved at {early_stopper.path}.")


    if os.path.exists(early_stopper.path):
        best_checkpoint = torch.load(early_stopper.path)
        model2.load_state_dict(best_checkpoint['model_state_dict'])
        del best_checkpoint  # ğŸ”¥ æ¸…ç†checkpoint
    else:
        print(f"Warning: Best model checkpoint not found at {early_stopper.path}. Using last epoch's model.")

    if writer:
        writer.close()
        del writer

    # ğŸ”¥ æ¸…ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸´æ—¶å˜é‡
    del criterion, scaler
    torch.cuda.empty_cache()

    print("=========================== FDIP_2 Training Finished ==================================")
    return model2


def train_fdip_3(model1, model2, model3, optimizer, scheduler, train_loader, val_loader, epochs, early_stopper,
                 start_epoch=0):
    """è®­ç»ƒ FDIP_3 æ¨¡å‹ï¼Œæ”¯æŒä»æŒ‡å®šè½®æ•°å¼€å§‹è®­ç»ƒ"""
    print("\n======================== Starting FDIP_3 Training (Online Inference)====================")
    criterion = nn.MSELoss()
    scaler = GradScaler()
    writer = SummaryWriter(os.path.join(LOG_RUN_DIR, 'ggip3')) if LOG_ENABLED else None

    for epoch in range(start_epoch, epochs):
        current_epoch = epoch + 1
        model1.eval()  # ä¸è®­ç»ƒ
        model2.eval()  # ä¸è®­ç»ƒ
        model3.train()
        train_losses = []
        epoch_pbar = tqdm(train_loader, desc=f"FDIP_3 Epoch {current_epoch}/{epochs}", leave=True)
        for data in epoch_pbar:
            acc, ori_6d, pose_6d_gt = [d.to(DEVICE, non_blocking=True).float() for d in
                                       (data[0], data[2], data[6])]  # pose_6d_gtæ˜¯24ä¸ªå…³èŠ‚çš„6Då§¿æ€

            with torch.no_grad():  # FDIP_1 å’Œ FDIP_2 çš„æ¨æ–­ä¸è®¡ç®—æ¢¯åº¦
                input1 = torch.cat((acc, ori_6d), -1).view(acc.shape[0], acc.shape[1], -1)
                p_leaf_logits = model1(input1)
                zeros = torch.zeros(p_leaf_logits.shape[0], p_leaf_logits.shape[1], 1, 3, device=DEVICE)
                p_leaf_pred = torch.cat(
                    [zeros, p_leaf_logits.view(p_leaf_logits.shape[0], p_leaf_logits.shape[1], 5, 3)], dim=2)

                input2 = torch.cat((acc, ori_6d, p_leaf_pred),-1).view(acc.shape[0], acc.shape[1], -1)
                p_all_pos_flattened = model2(input2)  # FDIP_2 è¾“å‡ºçš„æ‰€æœ‰24ä¸ªå…³èŠ‚çš„3Dä½ç½®ï¼Œå±•å¹³

            input_base = torch.cat((acc, ori_6d), -1).view(acc.shape[0], acc.shape[1], -1)  # FDIP_3 çš„ä¸€éƒ¨åˆ†è¾“å…¥

            # ç›®æ ‡æ˜¯æ‰€æœ‰24ä¸ªå…³èŠ‚çš„6Då§¿æ€ï¼Œå±•å¹³
            target = pose_6d_gt.view(pose_6d_gt.shape[0], pose_6d_gt.shape[1], -1)  # 24*6 = 144

            optimizer.zero_grad(set_to_none=True)
            with autocast():
                # FDIP_3 çš„è¾“å…¥æ˜¯åŸå§‹IMUæ•°æ®å’ŒFDIP_2é¢„æµ‹çš„æ‰€æœ‰å…³èŠ‚ä½ç½®
                pose_pred_flat = model3(input_base, p_all_pos_flattened)

                # é‡å¡‘ä¸ºä¾¿äºè®¡ç®—æŸå¤±çš„æ ¼å¼
                batch_size, seq_len = pose_pred_flat.shape[:2]
                pose_pred = pose_pred_flat.view(batch_size, seq_len, 24, 6)
                pose_gt = pose_6d_gt.view(batch_size, seq_len, 24, 6)

                # ä½¿ç”¨æ”¹è¿›çš„æŸå¤±å‡½æ•°
                loss = rotation_matrix_loss(pose_pred, pose_gt)

            if torch.isnan(loss) or torch.isinf(loss):
                print(f"Warning: NaN/Inf loss encountered at FDIP_3 Epoch {current_epoch}, skipping batch.")
                continue

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            train_losses.append(loss.item())
            epoch_pbar.set_postfix(loss=f"{loss.item():.4f}")

        # éªŒè¯é˜¶æ®µ
        model3.eval()
        val_losses = []
        with torch.no_grad():
            for data_val in val_loader:
                acc_val, ori_val, pose_6d_gt_val = [d.to(DEVICE, non_blocking=True).float() for d in
                                                    (data_val[0], data_val[2], data_val[6])]
                input1_val = torch.cat((acc_val, ori_val), -1).view(acc_val.shape[0], acc_val.shape[1], -1)
                p_leaf_logits_val = model1(input1_val)
                zeros_val = torch.zeros(p_leaf_logits_val.shape[0], p_leaf_logits_val.shape[1],3, device=DEVICE)
                p_leaf_pred_val = torch.cat(
                    [zeros_val, p_leaf_logits_val],
                    dim=2)

                input2_val = torch.cat(
                    (acc_val, ori_val, p_leaf_pred_val.view(p_leaf_pred_val.shape[0], p_leaf_pred_val.shape[1], 6, 3)),
                    -1).view(acc_val.shape[0], acc_val.shape[1], -1)
                p_all_pos_flattened_val = model2(input2_val)
                input_base_val = torch.cat((acc_val, ori_val), -1).view(acc_val.shape[0], acc_val.shape[1], -1)

                target_val = pose_6d_gt_val.view(pose_6d_gt_val.shape[0], pose_6d_gt_val.shape[1], -1)
                logits_val = model3(input_base_val, p_all_pos_flattened_val)
                loss_val = torch.sqrt(criterion(logits_val, target_val))

                if torch.isnan(loss_val) or torch.isinf(loss_val):
                    print(
                        f"Warning: NaN/Inf loss encountered at FDIP_1 Epoch {current_epoch}, skipping validation batch.")
                    continue

                val_losses.append(loss_val.item())

        avg_train_loss = np.mean(train_losses) if train_losses else 0.0
        avg_val_loss = np.mean(val_losses) if val_losses else 0.0
        current_lr = optimizer.param_groups[0]['lr']
        print(
            f'FDIP_3 Epoch {current_epoch}/{epochs} | Avg Train Loss: {avg_train_loss:.6f} | Avg Val Loss: {avg_val_loss:.6f} | LR: {current_lr:.6f}')

        if LOG_ENABLED and writer:
            writer.add_scalars('loss/fdip3', {'train': avg_train_loss, 'val': avg_val_loss}, current_epoch)
            writer.add_scalar('learning_rate/fdip3', current_lr, current_epoch)

        scheduler.step()
        early_stopper(avg_val_loss, model3, optimizer, current_epoch)
        if early_stopper.early_stop:
            print(f"Early stopping triggered at epoch {current_epoch} for FDIP_3.")
            break

        # ğŸ”¥ æ¯ä¸ªepochç»“æŸåæ¸…ç†å†…å­˜
        torch.cuda.empty_cache()

    print(
        f"FDIP_3 training finished. Loading best model from epoch {early_stopper.best_epoch} saved at {early_stopper.path}.")
    if os.path.exists(early_stopper.path):
        best_checkpoint = torch.load(early_stopper.path)
        model3.load_state_dict(best_checkpoint['model_state_dict'])
    else:
        print(f"Warning: Best model checkpoint not found at {early_stopper.path}. Using last epoch's model.")

    if writer:
        writer.close()
        del writer  # ğŸ”¥ æ¸…ç†writer

    # ğŸ”¥ æ¸…ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸´æ—¶å˜é‡
    del criterion, scaler
    torch.cuda.empty_cache()

    print("================================ FDIP_3 Training Finished =======================================")
    return model3


def clean_filename(filename):
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤Windowsä¸­ä¸å…è®¸çš„å­—ç¬¦"""
    invalid_chars = ['<', '>', ':', '"', '/', '\\', '|', '?', '*']
    for char in invalid_chars:
        filename = filename.replace(char, '_')
    return filename


def evaluate_pipeline(model1, model2, model3, data_loader):
    print("\n============================ Evaluating Complete Pipeline ======================================")

    # ğŸ”¥ è¯„ä¼°å‰æ¸…ç†å†…å­˜
    clear_memory()

    eval_results_dir = os.path.join("GGIP", f"evaluate_pipeline_{TIMESTAMP}")
    eval_plots_dir = os.path.join(eval_results_dir, "plots")
    eval_data_dir = os.path.join(eval_results_dir, "data")

    # åˆ›å»ºç›®å½•
    os.makedirs(eval_results_dir, exist_ok=True)
    os.makedirs(eval_plots_dir, exist_ok=True)
    os.makedirs(eval_data_dir, exist_ok=True)

    try:
        evaluator = PerFramePoseEvaluator()
        model1.eval()
        model2.eval()
        model3.eval()

        all_errors = {
            "pos_err": [],
            "mesh_err": [],
            "angle_err": [],
            "jitter_err": []
        }

        print("Running model evaluation...")
        with torch.no_grad():
            for data_val in tqdm(data_loader, desc="Evaluating Pipeline"):
                try:
                    # --- æ¨¡å‹å‰å‘ä¼ æ’­ ---
                    acc, ori_6d, pose_6d_gt = [d.to(DEVICE, non_blocking=True).float() for d in
                                               (data_val[0], data_val[2], data_val[6])]

                    input1 = torch.cat((acc, ori_6d), -1).view(acc.shape[0], acc.shape[1], -1)
                    p_leaf_logits = model1(input1)

                    zeros1 = torch.zeros(p_leaf_logits.shape[0], p_leaf_logits.shape[1], 3, device=DEVICE)
                    p_leaf_pred = torch.cat([zeros1, p_leaf_logits], dim=2)

                    input2 = torch.cat(
                        (acc, ori_6d, p_leaf_pred.view(p_leaf_pred.shape[0], p_leaf_pred.shape[1], 6, 3)),
                        -1).view(acc.shape[0], acc.shape[1], -1)
                    p_all_pos_flattened = model2(input2)
                    input_base = torch.cat((acc, ori_6d), -1).view(acc.shape[0], acc.shape[1], -1)
                    pose_pred_flat = model3(input_base, p_all_pos_flattened)

                    batch_size, seq_len = pose_pred_flat.shape[:2]
                    pose_pred = pose_pred_flat.view(batch_size, seq_len, 24, 6)

                    errs_dict = evaluator.eval(pose_pred, pose_6d_gt)

                    for key in all_errors.keys():
                        if errs_dict[key].numel() > 0:
                            all_errors[key].append(errs_dict[key].flatten().cpu())

                except Exception as e:
                    print(f"Warning: Error processing batch in evaluation: {e}")
                    continue

        # ğŸ”¥ è¯„ä¼°å‰æ¸…ç†å†…å­˜
        clear_memory()

        # --- æ±‡æ€»ç»“æœ ---
        if all_errors["mesh_err"]:
            print("Processing evaluation results...")

            # æ‹¼æ¥æ‰€æœ‰è¯¯å·®æ•°æ®
            final_errors = {key: torch.cat(val, dim=0) for key, val in all_errors.items() if val}
            avg_errors = {key: val.mean().item() for key, val in final_errors.items()}

            # æ‰“å°ç»“æœ
            print("\nComplete Pipeline Evaluation Results (Mean):")
            print(f"  - Positional Error (cm):      {avg_errors.get('pos_err', 'N/A'):.4f}")
            print(f"  - Mesh Error (cm):            {avg_errors.get('mesh_err', 'N/A'):.4f}")
            print(f"  - Angular Error (deg):        {avg_errors.get('angle_err', 'N/A'):.4f}")
            print(f"  - Jitter Error (cm/sÂ²):       {avg_errors.get('jitter_err', 'N/A'):.4f}")

            # --- ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶ ---
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            try:
                # 1. ä¿å­˜åŸå§‹è¯¯å·®æ•°æ® (pickleæ ¼å¼ï¼Œä¿æŒå®Œæ•´çš„tensoræ•°æ®)
                raw_data_path = os.path.join(eval_data_dir, f"raw_errors_{timestamp}.pkl")
                with open(raw_data_path, 'wb') as f:
                    pickle.dump(final_errors, f)
                print(f"Raw error data saved to: {raw_data_path}")

                # 2. ä¿å­˜ç»Ÿè®¡ç»“æœ (JSONæ ¼å¼ï¼Œæ˜“äºé˜…è¯»)
                stats_data = {
                    "timestamp": timestamp,
                    "evaluation_results": {
                        "mean_errors": avg_errors,
                        "sample_counts": {key: len(val) for key, val in final_errors.items()},
                        "std_errors": {key: val.std().item() for key, val in final_errors.items()},
                        "min_errors": {key: val.min().item() for key, val in final_errors.items()},
                        "max_errors": {key: val.max().item() for key, val in final_errors.items()}
                    },
                    "units": {  # æ·»åŠ å•ä½ä¿¡æ¯
                        "pos_err": "cm",
                        "mesh_err": "cm",
                        "angle_err": "degrees",
                        "jitter_err": "cm/sÂ²"
                    }
                }

                stats_path = os.path.join(eval_data_dir, f"evaluation_stats_{timestamp}.json")
                with open(stats_path, 'w') as f:
                    json.dump(stats_data, f, indent=2)
                print(f"Statistics saved to: {stats_path}")

                # 3. ä¿å­˜ä¸ºCSVæ ¼å¼ (æ–¹ä¾¿Excelæ‰“å¼€)
                csv_data = []
                for key, values in final_errors.items():
                    for value in values.numpy():
                        csv_data.append({
                            'metric': key,
                            'value': value,
                            'timestamp': timestamp
                        })

                if csv_data:
                    df = pd.DataFrame(csv_data)
                    csv_path = os.path.join(eval_data_dir, f"evaluation_data_{timestamp}.csv")
                    df.to_csv(csv_path, index=False)
                    print(f"CSV data saved to: {csv_path}")

            except Exception as e:
                print(f"Warning: Error saving data files: {e}")

            # --- ç”Ÿæˆå¹¶ä¿å­˜å›¾è¡¨ ---
            print("\nSaving error distribution plots...")

            # ä¿®æ­£åçš„å•ä½æ˜ å°„
            error_names_map = {
                "pos_err": "Positional Error (cm)",
                "mesh_err": "Mesh Error (cm)",
                "angle_err": "Angular Error (deg)",
                "jitter_err": "Jitter Error (cm/sÂ²)"  # ä¿®æ­£å•ä½
            }

            # å¯¹åº”çš„yè½´æ ‡ç­¾
            ylabel_map = {
                "pos_err": "Error (cm)",
                "mesh_err": "Error (cm)",
                "angle_err": "Error (degrees)",
                "jitter_err": "Error (cm/sÂ²)"  # ä¿®æ­£yè½´æ ‡ç­¾
            }

            for key, full_name in error_names_map.items():
                if key in final_errors:
                    try:
                        plt.figure(figsize=(8, 6))
                        sns.violinplot(data=final_errors[key].numpy(), color='skyblue', inner='box')

                        # ä½¿ç”¨æ­£ç¡®çš„æ ‡é¢˜å’Œyè½´æ ‡ç­¾
                        plt.title(f"{full_name} Distribution", fontsize=14, fontweight='bold')
                        plt.ylabel(ylabel_map[key], fontsize=12)  # ä½¿ç”¨å…·ä½“çš„å•ä½æ ‡ç­¾
                        plt.xlabel("Distribution", fontsize=12)

                        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯åˆ°å›¾ä¸Š
                        mean_val = final_errors[key].mean().item()
                        std_val = final_errors[key].std().item()
                        plt.text(0.02, 0.98, f'Mean: {mean_val:.2f}\nStd: {std_val:.2f}',
                                 transform=plt.gca().transAxes,
                                 verticalalignment='top',
                                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

                        # æ¸…ç†æ–‡ä»¶åï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
                        clean_name = clean_filename(
                            full_name.replace(' ', '_').replace('(', '').replace(')', '').replace('/', '_per_'))
                        filename = f"{clean_name}_violin_{timestamp}.png"
                        filepath = os.path.join(eval_plots_dir, filename)

                        plt.savefig(filepath, bbox_inches='tight', dpi=300)
                        plt.close()
                        print(f"  - Saved: {filepath}")

                    except Exception as e:
                        print(f"Warning: Error saving plot for {key}: {e}")
                        plt.close()
                        continue

            # --- ä¿å­˜æ±‡æ€»æŠ¥å‘Š ---
            try:
                report_path = os.path.join(eval_results_dir, f"evaluation_report_{timestamp}.txt")
                with open(report_path, 'w') as f:
                    f.write("=== GGIP Pipeline Evaluation Report ===\n")
                    f.write(f"Timestamp: {timestamp}\n")
                    f.write(f"Total samples evaluated: {len(final_errors.get('mesh_err', []))}\n\n")

                    f.write("Mean Errors:\n")
                    unit_labels = {"pos_err": "cm", "mesh_err": "cm", "angle_err": "deg", "jitter_err": "cm/sÂ²"}
                    for key, value in avg_errors.items():
                        unit = unit_labels.get(key, "")
                        f.write(f"  - {key}: {value:.4f} {unit}\n")

                    f.write("\nStandard Deviations:\n")
                    for key, val in final_errors.items():
                        unit = unit_labels.get(key, "")
                        f.write(f"  - {key}: {val.std().item():.4f} {unit}\n")

                    f.write("\nData Files Generated:\n")
                    f.write(f"  - Raw data: raw_errors_{timestamp}.pkl\n")
                    f.write(f"  - Statistics: evaluation_stats_{timestamp}.json\n")
                    f.write(f"  - CSV data: evaluation_data_{timestamp}.csv\n")
                    f.write(f"  - Plots: Located in plots/ subdirectory\n")

                print(f"Evaluation report saved to: {report_path}")

            except Exception as e:
                print(f"Warning: Error saving evaluation report: {e}")

        else:
            print("No evaluation results were generated.")

            # å³ä½¿æ²¡æœ‰ç»“æœä¹Ÿä¿å­˜ä¸€ä¸ªç©ºæŠ¥å‘Š
            try:
                empty_report_path = os.path.join(eval_results_dir,
                                                 f"evaluation_report_empty_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
                with open(empty_report_path, 'w') as f:
                    f.write("=== GGIP Pipeline Evaluation Report ===\n")
                    f.write(f"Timestamp: {datetime.now().strftime('%Y%m%d_%H%M%S')}\n")
                    f.write("Status: No evaluation results were generated.\n")
                    f.write("Possible reasons: Empty dataset, evaluation errors, or model issues.\n")
                print(f"Empty evaluation report saved to: {empty_report_path}")
            except Exception as e:
                print(f"Warning: Error saving empty report: {e}")

    except Exception as e:
        print(f"Critical error in evaluation pipeline: {e}")
        print("Continuing with main program execution...")

        # ä¿å­˜é”™è¯¯æŠ¥å‘Š
        try:
            error_report_path = os.path.join(eval_results_dir,
                                             f"evaluation_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
            with open(error_report_path, 'w') as f:
                f.write("=== GGIP Pipeline Evaluation Error Report ===\n")
                f.write(f"Timestamp: {datetime.now().strftime('%Y%m%d_%H%M%S')}\n")
                f.write(f"Error: {str(e)}\n")
                f.write(f"Error Type: {type(e).__name__}\n")
            print(f"Error report saved to: {error_report_path}")
        except:
            print("Could not save error report")

    print(f"\nEvaluation completed. Results saved in: {eval_results_dir}")


def rotation_matrix_loss(pred_6d, target_6d):
    """
    å°†6Dè¡¨ç¤ºè½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µåè®¡ç®—FrobeniusèŒƒæ•°æŸå¤±

    å‚æ•°:
        pred_6d: é¢„æµ‹çš„6Dæ—‹è½¬è¡¨ç¤ºï¼Œå½¢çŠ¶ä¸º[B, S, J, 6]
        target_6d: ç›®æ ‡6Dæ—‹è½¬è¡¨ç¤ºï¼Œå½¢çŠ¶ä¸º[B, S, J, 6]
    è¿”å›:
        æ—‹è½¬çŸ©é˜µç©ºé—´ä¸­çš„æŸå¤±
    """
    # å±•å¹³ä»¥ä¾¿æ‰¹é‡å¤„ç†
    batch_size, seq_len, joints, _ = pred_6d.shape
    pred_6d_flat = pred_6d.reshape(-1, 6)
    target_6d_flat = target_6d.reshape(-1, 6)

    # ä½¿ç”¨æ‚¨æä¾›çš„å‡½æ•°è½¬æ¢ä¸ºæ—‹è½¬çŸ©é˜µ
    pred_rotmat = r6d_to_rotation_matrix(pred_6d_flat)  # è¾“å‡ºå½¢çŠ¶ [B*S*J, 3, 3]
    target_rotmat = r6d_to_rotation_matrix(target_6d_flat)

    # è®¡ç®—FrobeniusèŒƒæ•° (çŸ©é˜µå…ƒç´ é—´çš„æ¬§å‡ é‡Œå¾·è·ç¦»)
    loss = torch.mean(torch.norm(pred_rotmat - target_rotmat, dim=(-2, -1)))

    return loss

def parse_args():
    parser = argparse.ArgumentParser(description='FDIP Training')
    parser.add_argument('--resume', type=str, default=None,
                       help='Resume training from checkpoint directory (e.g., 20250804_143022)')
    parser.add_argument('--checkpoint_dir', type=str, default=None,
                       help='Specific checkpoint directory path')
    return parser.parse_args()
# python train.py --resume 20250804_143022
# python train.py --checkpoint_dir GGIP/checkpoints_20250804_143022


def main():
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œå®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹ï¼Œå¹¶æ”¯æŒä»æ–­ç‚¹æ¢å¤ã€‚"""
    set_seed(SEED)
    print("==================== Starting Full Training Pipeline =====================")

    args = parse_args()
    setup_directories_and_paths(args)
    create_directories()
    try:
        # train_loader, val_loader = load_data_separate()
        train_loader, val_loader = load_data_unified_split(
            train_percent=0.8,
            val_percent=0.2,
            seed=SEED
        )
        print("âœ“ Using separate train/validation datasets - No data leakage risk!")

        print("âœ“ Using unified dataset with consistent split!")
        # æ£€æŸ¥æ•°æ®åˆ†å¸ƒä¸€è‡´æ€§
        check_data_distribution(train_loader, val_loader)

    except Exception as e:
        print(f"âŒ Failed to load separate datasets: {e}")
        print("âš ï¸  Falling back to legacy split method (may have data leakage risk)")
        train_loader, val_loader = load_data_legacy(train_percent=0.9)

    patience = PATIENCE
    max_epochs = MAX_EPOCHS

    total_start_time = time.time()

    # --- é˜¶æ®µ 1: FDIP_1 ---
    print("\n--- Initializing Stage 1: FDIP_1 ---")
    model1 = FDIP_1(input_dim=6 * 9, output_dim=5 * 3).to(DEVICE)
    checkpoint_path1 = os.path.join(CHECKPOINT_DIR, 'ggip1', 'best_model_fdip1.pth')
    # å®šä¹‰é˜¶æ®µ1çš„å®Œæˆæ ‡è®°æ–‡ä»¶è·¯å¾„
    completion_marker1 = os.path.join(CHECKPOINT_DIR, 'ggip1', 'fdip1_completed.marker')

    # æ£€æŸ¥é˜¶æ®µ1æ˜¯å¦å·²ç»å®Œæˆ
    if os.path.exists(completion_marker1):
        print("Stage 1 (FDIP_1) already completed. Loading best model and skipping training.")
        if os.path.exists(checkpoint_path1):
            checkpoint = torch.load(checkpoint_path1, map_location=DEVICE)
            model1.load_state_dict(checkpoint['model_state_dict'])
            print(f"Successfully loaded best model for FDIP_1 from {checkpoint_path1}")
            del checkpoint  # æ¸…ç†checkpoint
        else:
            print(f"Error: Completion marker found, but checkpoint file {checkpoint_path1} is missing!")
            print("Please resolve this inconsistency or remove the marker file to re-train.")
            sys.exit(1)  # ç»ˆæ­¢ç¨‹åºï¼Œå› ä¸ºçŠ¶æ€ä¸ä¸€è‡´
    else:
        # ä¼˜åŒ–å™¨å†³å®šå¦‚ä½•æ ¹æ®æ¢¯åº¦æ›´æ–°å‚æ•°ã€è°ƒåº¦å™¨å†³å®šä½•æ—¶è°ƒæ•´å­¦ä¹ ç‡å¤§å°
        optimizer1 = optim.Adam(model1.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
        scheduler1 = optim.lr_scheduler.CosineAnnealingLR(optimizer1, T_max=max_epochs, eta_min=1e-6)
        early_stopper1 = EarlyStopping(patience=patience, path=checkpoint_path1, verbose=True)
        start_epoch1 = 0

        if os.path.exists(checkpoint_path1):
            print(f"Found checkpoint for FDIP_1. Resuming training from: {checkpoint_path1}")
            checkpoint = torch.load(checkpoint_path1, map_location=DEVICE)
            model1.load_state_dict(checkpoint['model_state_dict'])
            optimizer1.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch1 = checkpoint['epoch']
            early_stopper1.val_loss_min = checkpoint['val_loss_min']
            early_stopper1.best_score = checkpoint['best_score']
            early_stopper1.counter = checkpoint.get('early_stopping_counter', 0)
            for _ in range(start_epoch1):
                scheduler1.step()
            print(
                f"Resuming from Epoch {start_epoch1 + 1}. Best validation loss so far: {early_stopper1.val_loss_min:.6f}")
            del checkpoint  # æ¸…ç†checkpoint
        else:
            print("No checkpoint found for FDIP_1. Starting training from scratch.")

        model1 = train_fdip_1(
            model=model1,
            optimizer=optimizer1,
            scheduler=scheduler1,
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=max_epochs,
            early_stopper=early_stopper1,
            start_epoch=start_epoch1
        )

        # è®­ç»ƒæˆåŠŸç»“æŸåï¼Œåˆ›å»ºå®Œæˆæ ‡è®°æ–‡ä»¶
        with open(completion_marker1, 'w') as f:
            f.write(f"Completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(
                f"Best model saved at epoch {early_stopper1.best_epoch} with val_loss {early_stopper1.val_loss_min:.6f}\n")
        print(f"Stage 1 (FDIP_1) marked as completed.")

        # ğŸ”¥ æ¸…ç†Stage 1çš„è®­ç»ƒå¯¹è±¡ï¼Œé‡Šæ”¾å†…å­˜
        cleanup_training_objects(optimizer1, scheduler1, early_stopper1)
        print("Stage 1 training objects cleaned up.")

    # --- é˜¶æ®µ 2: FDIP_2 ---
    print("\n--- Initializing Stage 2: FDIP_2 ---")
    model2 = FDIP_2(input_dim=6 * 12, output_dim=24 * 3).to(DEVICE)
    checkpoint_path2 = os.path.join(CHECKPOINT_DIR, 'ggip2', 'best_model_fdip2.pth')
    completion_marker2 = os.path.join(CHECKPOINT_DIR, 'ggip2', 'fdip2_completed.marker')

    if os.path.exists(completion_marker2):
        print("Stage 2 (FDIP_2) already completed. Loading best model and skipping training.")
        if os.path.exists(checkpoint_path2):
            checkpoint = torch.load(checkpoint_path2, map_location=DEVICE)
            model2.load_state_dict(checkpoint['model_state_dict'])
            print(f"Successfully loaded best model for FDIP_2 from {checkpoint_path2}")
            del checkpoint  # æ¸…ç†checkpoint
        else:
            print(f"Error: Completion marker found, but checkpoint file {checkpoint_path2} is missing!")
            sys.exit(1)
    else:
        optimizer2 = optim.Adam(model2.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
        scheduler2 = optim.lr_scheduler.CosineAnnealingLR(optimizer2, T_max=max_epochs, eta_min=1e-6)
        early_stopper2 = EarlyStopping(patience=patience, path=checkpoint_path2, verbose=True)
        start_epoch2 = 0

        if os.path.exists(checkpoint_path2):
            print(f"Found checkpoint for FDIP_2. Resuming training from: {checkpoint_path2}")
            checkpoint = torch.load(checkpoint_path2, map_location=DEVICE)
            model2.load_state_dict(checkpoint['model_state_dict'])
            optimizer2.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch2 = checkpoint['epoch']
            early_stopper2.val_loss_min = checkpoint['val_loss_min']
            early_stopper2.best_score = checkpoint['best_score']
            early_stopper2.counter = checkpoint.get('early_stopping_counter', 0)
            for _ in range(start_epoch2):
                scheduler2.step()
            print(
                f"Resuming from Epoch {start_epoch2 + 1}. Best validation loss so far: {early_stopper2.val_loss_min:.6f}")
            del checkpoint  # æ¸…ç†checkpoint
        else:
            print("No checkpoint found for FDIP_2. Starting training from scratch.")

        model2 = train_fdip_2(
            model1=model1,
            model2=model2,
            optimizer=optimizer2,
            scheduler=scheduler2,
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=max_epochs,
            early_stopper=early_stopper2,
            start_epoch=start_epoch2
        )

        with open(completion_marker2, 'w') as f:
            f.write(f"Completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(
                f"Best model saved at epoch {early_stopper2.best_epoch} with val_loss {early_stopper2.val_loss_min:.6f}\n")
        print(f"Stage 2 (FDIP_2) marked as completed.")

        # ğŸ”¥ æ¸…ç†Stage 2çš„è®­ç»ƒå¯¹è±¡ï¼Œé‡Šæ”¾å†…å­˜
        cleanup_training_objects(optimizer2, scheduler2, early_stopper2)
        print("Stage 2 training objects cleaned up.")

    # --- é˜¶æ®µ 3: FDIP_3 ---
    print("\n--- Initializing Stage 3: FDIP_3 ---")
    model3 = FDIP_3(input_dim=288, output_dim=24 * 6).to(DEVICE)
    checkpoint_path3 = os.path.join(CHECKPOINT_DIR, 'ggip3', 'best_model_fdip3.pth')
    completion_marker3 = os.path.join(CHECKPOINT_DIR, 'ggip3', 'fdip3_completed.marker')

    if os.path.exists(completion_marker3):
        print("Stage 3 (FDIP_3) already completed. Loading best model and skipping training.")
        if os.path.exists(checkpoint_path3):
            checkpoint = torch.load(checkpoint_path3, map_location=DEVICE)
            model3.load_state_dict(checkpoint['model_state_dict'])
            print(f"Successfully loaded best model for FDIP_3 from {checkpoint_path3}")
            del checkpoint  # æ¸…ç†checkpoint
        else:
            print(f"Error: Completion marker found, but checkpoint file {checkpoint_path3} is missing!")
            sys.exit(1)
    else:
        optimizer3 = optim.Adam(model3.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
        scheduler3 = optim.lr_scheduler.CosineAnnealingLR(optimizer3, T_max=max_epochs, eta_min=1e-6)
        early_stopper3 = EarlyStopping(patience=patience, path=checkpoint_path3, verbose=True)
        start_epoch3 = 0

        if os.path.exists(checkpoint_path3):
            print(f"Found checkpoint for FDIP_3. Resuming training from: {checkpoint_path3}")
            checkpoint = torch.load(checkpoint_path3, map_location=DEVICE)
            model3.load_state_dict(checkpoint['model_state_dict'])
            optimizer3.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch3 = checkpoint['epoch']
            early_stopper3.val_loss_min = checkpoint['val_loss_min']
            early_stopper3.best_score = checkpoint['best_score']
            early_stopper3.counter = checkpoint.get('early_stopping_counter', 0)
            for _ in range(start_epoch3):
                scheduler3.step()
            print(
                f"Resuming from Epoch {start_epoch3 + 1}. Best validation loss so far: {early_stopper3.val_loss_min:.6f}")
            del checkpoint  # æ¸…ç†checkpoint
        else:
            print("No checkpoint found for FDIP_3. Starting training from scratch.")

        model3 = train_fdip_3(
            model1=model1,
            model2=model2,
            model3=model3,
            optimizer=optimizer3,
            scheduler=scheduler3,
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=max_epochs,
            early_stopper=early_stopper3,
            start_epoch=start_epoch3
        )

        with open(completion_marker3, 'w') as f:
            f.write(f"Completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(
                f"Best model saved at epoch {early_stopper3.best_epoch} with val_loss {early_stopper3.val_loss_min:.6f}\n")
        print(f"Stage 3 (FDIP_3) marked as completed.")

        # ğŸ”¥ æ¸…ç†Stage 3çš„è®­ç»ƒå¯¹è±¡ï¼Œé‡Šæ”¾å†…å­˜
        cleanup_training_objects(optimizer3, scheduler3, early_stopper3)
        print("Stage 3 training objects cleaned up.")

    print("\nAll training stages complete!")
    total_end_time = time.time()
    print(f"Total training time: {(total_end_time - total_start_time) / 3600:.2f} hours")

    # æœ€ç»ˆè¯„ä¼°å‰å†æ¬¡æ¸…ç†å†…å­˜
    clear_memory()
    evaluate_pipeline(model1, model2, model3, val_loader)

    print("\nTraining and evaluation finished successfully!")


if __name__ == '__main__':
    main()
