import os
import sys

import inspect

# === getargspecå…¼å®¹æ€§ä¿®å¤ ===
if not hasattr(inspect, 'getargspec'):
    def getargspec_compat(func):
        """å…¼å®¹æ€§åŒ…è£…å™¨ï¼Œæ›¿ä»£å·²å¼ƒç”¨çš„getargspec"""
        try:
            sig = inspect.signature(func)
            args = list(sig.parameters.keys())
            defaults = []
            varargs = None
            keywords = None

            for param in sig.parameters.values():
                if param.kind == inspect.Parameter.VAR_POSITIONAL:
                    varargs = param.name
                elif param.kind == inspect.Parameter.VAR_KEYWORD:
                    keywords = param.name
                elif param.default != inspect.Parameter.empty:
                    defaults.append(param.default)

            # æ„é€ ä¸getargspecç›¸åŒçš„è¿”å›æ ¼å¼
            from collections import namedtuple
            ArgSpec = namedtuple('ArgSpec', 'args varargs keywords defaults')
            return ArgSpec(
                args=args,
                varargs=varargs,
                keywords=keywords,
                defaults=tuple(defaults) if defaults else None
            )
        except Exception as e:
            print(f"Warning: getargspec compatibility failed for {func}: {e}")
            from collections import namedtuple
            ArgSpec = namedtuple('ArgSpec', 'args varargs keywords defaults')
            return ArgSpec(args=[], varargs=None, keywords=None, defaults=None)


    inspect.getargspec = getargspec_compat
    print("âœ… Applied inspect.getargspec compatibility patch for Python 3.11+")


import time
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from torch.amp import autocast, GradScaler
from torch.utils.data import DataLoader, Subset
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import json
import pickle
import pandas as pd
from datetime import datetime
from articulate.math import r6d_to_rotation_matrix
from data.dataset_posReg import ImuDataset
from model.net_zd import FDIP_1, FDIP_2, FDIP_3
from evaluator import PoseEvaluator, PerFramePoseEvaluator
import gc
import argparse

# --- Configuration ---
os.environ["CUDA_VISIBLE_DEVICES"] = '0'
LEARNING_RATE = 5e-6
WEIGHT_DECAY = 5e-3
BATCH_SIZE = 64
DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
LOG_ENABLED = True
TRAIN_PERCENT = 0.9
BATCH_SIZE_VAL = 32
SEED = 42
PATIENCE = 20
MAX_EPOCHS = 150
DELTA = 0

# --- Paths ---
TRAIN_DATA_FOLDERS = [
    os.path.join("F:\\", "IMUdata", "TotalCapture_Real_60FPS", "KaPt"),
    os.path.join("F:\\", "IMUdata", "DIPIMUandOthers", "DIP_6"),
    os.path.join("F:\\", "IMUdata", "AMASS", "DanceDB", "pt"),
    os.path.join("F:\\", "IMUdata", "AMASS", "HumanEva", "pt"),
]
VAL_DATA_FOLDERS = [
    os.path.join("F:\\", "IMUdata", "SingleOne", "pt"),
]

TIMESTAMP = None
CHECKPOINT_DIR = None
LOG_DIR = "log"
LOG_RUN_DIR = None
TRAINING_MODE = None


def parse_args():
    parser = argparse.ArgumentParser(description='Improved FDIP Training with Acceleration Normalization Only')
    parser.add_argument('--resume', type=str, default=None,
                        help='Resume training from checkpoint directory (e.g., 20250804_143022_joint)')
    parser.add_argument('--checkpoint_dir', type=str, default=None,
                        help='Specific checkpoint directory path')
    parser.add_argument('--use_joint_training', action='store_true', default=True,
                        help='Use end-to-end joint training (default: True)')
    parser.add_argument('--use_residual', action='store_true', default=False,
                        help='Use residual connections in models (default: False)')
    parser.add_argument('--batch_size', type=int, default=64,
                        help='Training batch size (default: 64)')
    parser.add_argument('--learning_rate', type=float, default=1e-4,
                        help='Learning rate (default: 5e-5)')
    parser.add_argument('--max_epochs', type=int, default=150,
                        help='Maximum training epochs (default: 150)')
    parser.add_argument('--patience', type=int, default=20,
                        help='Early stopping patience (default: 20)')

    # æ–°å¢å‚æ•°ï¼šç»­è®­æ—¶æ˜¯å¦é‡ç½®æœ€ä½³æŸå¤±è®°å½•
    parser.add_argument('--reset_best_on_resume', action='store_true', default=False,
                        help='Reset best validation loss when resuming training. Useful when loss weights are changed.')

    return parser.parse_args()


class AccelerationNormalizer:
    """åªå¯¹åŠ é€Ÿåº¦æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–çš„ç±»"""

    def __init__(self):
        self.acc_mean = None
        self.acc_std = None
        self.fitted = False

    def fit(self, train_loader, max_batches=50):
        """åœ¨è®­ç»ƒæ•°æ®ä¸Šè®¡ç®—åŠ é€Ÿåº¦ç»Ÿè®¡é‡"""
        print("Computing acceleration normalization statistics...")
        acc_vals = []

        batch_count = 0
        for data in tqdm(train_loader, desc="Computing acc stats"):
            if batch_count >= max_batches:
                break

            try:
                acc = data[0].float()  # åªå–åŠ é€Ÿåº¦æ•°æ®

                # åŸºç¡€æ•°æ®è´¨é‡æ£€æŸ¥
                if not torch.isnan(acc).any() and not torch.isinf(acc).any() and acc.abs().max() < 100:
                    acc_vals.append(acc)
                    batch_count += 1
                else:
                    continue

            except Exception as e:
                print(f"Warning: Error in batch {batch_count}: {e}")
                continue

        if not acc_vals:
            print("Warning: No valid acceleration data found for normalization!")
            return False

        # åˆå¹¶æ‰€æœ‰åŠ é€Ÿåº¦æ•°æ®
        acc_all = torch.cat(acc_vals, dim=0)  # [Total_samples, seq_len, joint_num, 3]

        # è®¡ç®—åŠ é€Ÿåº¦çš„å‡å€¼å’Œæ ‡å‡†å·® - å¯¹æ¯ä¸ªå…³èŠ‚æ¯ä¸ªåæ ‡è½´ç‹¬ç«‹è®¡ç®—
        self.acc_mean = acc_all.mean(dim=(0, 1), keepdim=True)  # [1, 1, joint_num, 3]
        self.acc_std = acc_all.std(dim=(0, 1), keepdim=True) + 1e-8

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"ğŸ“Š Acceleration Normalization Statistics:")
        print(f"   - Data shape: {acc_all.shape}")
        print(f"   - Acc range: [{acc_all.min():.3f}, {acc_all.max():.3f}]")
        print(f"   - Mean shape: {self.acc_mean.shape}")
        print(f"   - Global mean: {self.acc_mean.mean():.4f}")
        print(f"   - Global std: {self.acc_std.mean():.4f}")

        # æ£€æŸ¥æ¯ä¸ªå…³èŠ‚çš„ç»Ÿè®¡é‡
        num_joints = acc_all.shape[2]
        for joint_idx in range(min(num_joints, 3)):  # åªæ˜¾ç¤ºå‰3ä¸ªå…³èŠ‚çš„ä¿¡æ¯
            joint_mean = self.acc_mean[0, 0, joint_idx].mean().item()
            joint_std = self.acc_std[0, 0, joint_idx].mean().item()
            print(f"   - Joint {joint_idx}: mean={joint_mean:.4f}, std={joint_std:.4f}")

        self.fitted = True

        # æ¸…ç†å†…å­˜
        del acc_vals, acc_all
        torch.cuda.empty_cache()

        return True

    def normalize_acc(self, acc):
        """åªæ ‡å‡†åŒ–åŠ é€Ÿåº¦æ•°æ®"""
        if not self.fitted:
            print("Warning: Normalizer not fitted yet, returning original data")
            return acc

        acc_norm = (acc - self.acc_mean.to(acc.device)) / (self.acc_std.to(acc.device) + 1e-8)
        return acc_norm

    def denormalize_acc(self, acc_norm):
        """åæ ‡å‡†åŒ–åŠ é€Ÿåº¦æ•°æ®"""
        if not self.fitted:
            return acc_norm
        return acc_norm * self.acc_std.to(acc_norm.device) + self.acc_mean.to(acc_norm.device)

    def save(self, path):
        """ä¿å­˜æ ‡å‡†åŒ–å‚æ•°"""
        if self.fitted:
            torch.save({
                'acc_mean': self.acc_mean,
                'acc_std': self.acc_std,
                'fitted': self.fitted
            }, path)
            print(f"ğŸ“ Acceleration normalizer saved to: {path}")

    def load(self, path):
        """åŠ è½½æ ‡å‡†åŒ–å‚æ•°"""
        if os.path.exists(path):
            checkpoint = torch.load(path, map_location='cpu')
            self.acc_mean = checkpoint['acc_mean']
            self.acc_std = checkpoint['acc_std']
            self.fitted = checkpoint.get('fitted', True)
            print(f"ğŸ“ Acceleration normalizer loaded from: {path}")
            return True
        return False


def improved_data_check(acc, ori_6d, pos=None):
    """æ”¹è¿›çš„æ•°æ®è´¨é‡æ£€æŸ¥"""
    # æ›´å®½æ¾çš„æå€¼æ£€æŸ¥
    acc_max_threshold = 430.0
    ori_max_threshold = 10.0
    pos_max_threshold = 50.0

    # NaN/Infæ£€æŸ¥
    if (torch.isnan(acc).any() or torch.isnan(ori_6d).any() or
            torch.isinf(acc).any() or torch.isinf(ori_6d).any()):
        return False, "NaN/Inf detected in acc/ori"

    if pos is not None:
        if torch.isnan(pos).any() or torch.isinf(pos).any():
            return False, "NaN/Inf in position data"
        if pos.abs().max() > pos_max_threshold:
            return False, f"Extreme pos values: {pos.abs().max():.3f}"

    # æå€¼æ£€æŸ¥
    if acc.abs().max() > acc_max_threshold:
        return False, f"Extreme acc values: {acc.abs().max():.3f}"

    if ori_6d.abs().max() > ori_max_threshold:
        return False, f"Extreme ori values: {ori_6d.abs().max():.3f}"

    return True, "OK"


# ===== æ”¹è¿›çš„æ¨¡å‹ç±» =====
class FDIP_2_Residual(nn.Module):
    """æ”¹è¿›çš„FDIP_2ï¼ŒåŠ å…¥æ®‹å·®è¿æ¥å’Œé—¨æ§æœºåˆ¶"""

    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.backbone = FDIP_2(input_dim, output_dim)

        # æ®‹å·®è¿æ¥å±‚
        self.residual_proj = nn.Sequential(
            nn.Linear(input_dim, input_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.05),
            nn.Linear(input_dim // 2, output_dim)
        )

        # é—¨æ§èåˆ
        self.gate = nn.Sequential(
            nn.Linear(output_dim, output_dim),
            nn.Sigmoid()
        )

        # å±‚å½’ä¸€åŒ–
        self.layer_norm = nn.LayerNorm(output_dim)

    def forward(self, x):
        # ä¸»åˆ†æ”¯è¾“å‡º
        main_output = self.backbone(x)

        # æ®‹å·®åˆ†æ”¯
        # å¯¹åºåˆ—ç»´åº¦è¿›è¡Œæ± åŒ–ä»¥å‡å°‘è®¡ç®—é‡
        x_pooled = x.mean(dim=1, keepdim=True).expand(-1, x.shape[1], -1)
        residual = self.residual_proj(x_pooled)

        # é—¨æ§èåˆ
        gate_weight = self.gate(main_output)
        output = gate_weight * main_output + (1 - gate_weight) * residual

        # å±‚å½’ä¸€åŒ–
        output = self.layer_norm(output)

        return output


class FDIP_3_Residual(nn.Module):
    """æ”¹è¿›çš„FDIP_3ï¼ŒåŠ å…¥æ®‹å·®è¿æ¥å’Œæ³¨æ„åŠ›æœºåˆ¶"""

    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.backbone = FDIP_3(input_dim, output_dim)

        # ç‰¹å¾ç»´åº¦
        self.feature_dim = 256

        # å¤šå¤´æ³¨æ„åŠ›ï¼Œç”¨äºèåˆIMUå’Œä½ç½®ä¿¡æ¯
        self.attention = nn.MultiheadAttention(
            embed_dim=self.feature_dim,
            num_heads=8,
            batch_first=True,
            dropout=0.1
        )

        self.norm1 = nn.LayerNorm(self.feature_dim)
        self.norm2 = nn.LayerNorm(self.feature_dim)

        # æŠ•å½±å±‚
        self.imu_proj = nn.Sequential(
            nn.Linear(input_dim, self.feature_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

        self.pos_proj = nn.Sequential(
            nn.Linear(72, self.feature_dim),  # 24*3 -> feature_dim
            nn.ReLU(),
            nn.Dropout(0.1)
        )

        self.output_proj = nn.Sequential(
            nn.Linear(self.feature_dim, self.feature_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(self.feature_dim // 2, output_dim)
        )

        # èåˆæƒé‡ï¼ˆå¯å­¦ä¹ ï¼‰
        self.fusion_weight = nn.Parameter(torch.tensor(0.7))

    def forward(self, imu_input, pos_input):
        # åŸå§‹è¾“å‡º
        main_output = self.backbone(imu_input, pos_input)

        # æ³¨æ„åŠ›å¢å¼ºåˆ†æ”¯
        imu_features = self.imu_proj(imu_input)  # [B, S, feature_dim]
        pos_features = self.pos_proj(pos_input)  # [B, S, feature_dim]

        # è‡ªæ³¨æ„åŠ› - IMUç‰¹å¾
        attended_imu, _ = self.attention(imu_features, imu_features, imu_features)
        attended_imu = self.norm1(attended_imu + imu_features)

        # äº¤å‰æ³¨æ„åŠ› - IMUä¸ä½ç½®ç‰¹å¾
        cross_attended, _ = self.attention(attended_imu, pos_features, pos_features)
        enhanced_features = self.norm2(cross_attended + attended_imu)

        # è¾“å‡ºæŠ•å½±
        attention_output = self.output_proj(enhanced_features)

        # è‡ªé€‚åº”èåˆ
        fusion_weight = torch.sigmoid(self.fusion_weight)
        final_output = fusion_weight * main_output + (1 - fusion_weight) * attention_output

        return final_output


# ===== æ”¹è¿›çš„æ—©åœç±» =====
class MultiModelEarlyStopping:
    """æ”¯æŒå¤šæ¨¡å‹çš„æ—©åœæœºåˆ¶"""

    def __init__(self, patience=10, verbose=True, delta=0, path='checkpoint.pt', reset_best=False):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.inf
        self.delta = delta
        self.path = path
        self.best_epoch = 0
        self.reset_best = reset_best  # æ–°å¢ï¼šæ˜¯å¦é‡ç½®æœ€ä½³è®°å½•

    def reset_best_values(self, current_epoch):
        """é‡ç½®æœ€ä½³å€¼ï¼Œç”¨äºç»­è®­æ—¶å¤„ç†æŸå¤±æƒé‡å˜åŒ–çš„æƒ…å†µ"""
        if self.verbose:
            print(f"ğŸ”„ Resetting best validation loss due to training configuration changes...")
            print(f"   Previous best: {self.val_loss_min:.6f} at epoch {self.best_epoch}")

        self.best_score = None
        self.val_loss_min = np.inf
        self.counter = 0
        self.best_epoch = current_epoch

        if self.verbose:
            print(f"   Reset complete. Will save next valid validation loss as new best.")

    def __call__(self, val_loss, models, optimizer, epoch, normalizer=None):
        if not np.isfinite(val_loss) or val_loss == 0.0:
            if self.verbose:
                print(f"Warning: Validation loss is {val_loss} at epoch {epoch}, skipping EarlyStopping.")
            return

        score = -val_loss

        # å¦‚æœæ˜¯é‡ç½®æ¨¡å¼ä¸”è¿™æ˜¯ç¬¬ä¸€æ¬¡è°ƒç”¨
        if self.reset_best and self.best_score is None and self.val_loss_min == np.inf:
            if self.verbose:
                print(f"ğŸ†• First validation after reset - accepting current loss {val_loss:.6f} as new best")
            self.best_score = score
            self.save_checkpoint(val_loss, models, optimizer, epoch, normalizer)
            return

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, models, optimizer, epoch, normalizer)
        elif score < self.best_score + self.delta:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
                print(f'Current: {val_loss:.6f} vs Best: {self.val_loss_min:.6f}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, models, optimizer, epoch, normalizer)
            self.counter = 0

    def save_checkpoint(self, val_loss, models, optimizer, epoch, normalizer=None):
        """ä¿å­˜å¤šæ¨¡å‹æ£€æŸ¥ç‚¹"""
        if self.verbose:
            improvement = self.val_loss_min - val_loss if self.val_loss_min != np.inf else 0
            print(f'âœ… Validation loss {"improved" if improvement > 0 else "updated"} '
                  f'({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving checkpoint to {self.path}...')

        os.makedirs(os.path.dirname(self.path), exist_ok=True)

        checkpoint = {
            'epoch': epoch,
            'model1_state_dict': models[0].state_dict(),
            'model2_state_dict': models[1].state_dict(),
            'model3_state_dict': models[2].state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_loss_min': val_loss,
            'best_score': self.best_score,
            'early_stopping_counter': self.counter,
            'training_mode': TRAINING_MODE,
            'reset_best': self.reset_best  # ä¿å­˜é‡ç½®æ ‡å¿—
        }
        torch.save(checkpoint, self.path)

        # å•ç‹¬ä¿å­˜æ ‡å‡†åŒ–å™¨
        if normalizer is not None and normalizer.fitted:
            norm_path = self.path.replace('.pth', '_normalizer.pth')
            normalizer.save(norm_path)

        self.val_loss_min = val_loss
        self.best_epoch = epoch


# ===== å·¥å…·å‡½æ•° =====

def create_directories():
    """åˆ›å»ºå¿…è¦çš„ç›®å½•ï¼Œæ ¹æ®è®­ç»ƒæ¨¡å¼åŒºåˆ†"""
    if TRAINING_MODE == "joint":
        dirs = [
            os.path.join(CHECKPOINT_DIR, "joint_e2e_training"),
            os.path.join(CHECKPOINT_DIR, "joint_e2e_logs"),
            LOG_DIR,
            LOG_RUN_DIR,
        ]
    else:
        dirs = [
            os.path.join(CHECKPOINT_DIR, "sequential_stage1"),
            os.path.join(CHECKPOINT_DIR, "sequential_stage2"),
            os.path.join(CHECKPOINT_DIR, "sequential_stage3"),
            LOG_DIR,
            LOG_RUN_DIR,
        ]

    for dir_path in dirs:
        os.makedirs(dir_path, exist_ok=True)

    print(f"Directories created with timestamp {TIMESTAMP} (Mode: {TRAINING_MODE}):")
    for dir_path in dirs:
        print(f"  - {dir_path}")


def set_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    print(f"Random seed set to {seed}")


def setup_directories_and_paths(args):
    """è®¾ç½®å…¨å±€è·¯å¾„å˜é‡ï¼ŒåŒ…å«è®­ç»ƒæ¨¡å¼åŒºåˆ†"""
    global TIMESTAMP, CHECKPOINT_DIR, LOG_RUN_DIR, TRAINING_MODE

    TRAINING_MODE = "joint" if args.use_joint_training else "sequential"

    if args.resume:
        TIMESTAMP = args.resume
        base_checkpoint_dir = os.path.join("GGIP", f"checkpoints_{TIMESTAMP}")
        if os.path.exists(os.path.join(base_checkpoint_dir, "joint_e2e_training")):
            TRAINING_MODE = "joint"
        elif os.path.exists(os.path.join(base_checkpoint_dir, "sequential_stage1")):
            TRAINING_MODE = "sequential"

        CHECKPOINT_DIR = base_checkpoint_dir
        print(f"Resuming training from timestamp: {TIMESTAMP} (Mode: {TRAINING_MODE})")

    elif args.checkpoint_dir:
        CHECKPOINT_DIR = args.checkpoint_dir
        TIMESTAMP = os.path.basename(CHECKPOINT_DIR).replace("checkpoints_", "")
        if os.path.exists(os.path.join(CHECKPOINT_DIR, "joint_e2e_training")):
            TRAINING_MODE = "joint"
        elif os.path.exists(os.path.join(CHECKPOINT_DIR, "sequential_stage1")):
            TRAINING_MODE = "sequential"
        print(f"Using checkpoint directory: {CHECKPOINT_DIR} (Mode: {TRAINING_MODE})")

    else:
        base_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        mode_suffix = "joint" if TRAINING_MODE == "joint" else "seq"
        TIMESTAMP = f"{base_timestamp}_{mode_suffix}"
        CHECKPOINT_DIR = os.path.join("GGIP", f"checkpoints_{TIMESTAMP}")
        print(f"Starting new {TRAINING_MODE} training with timestamp: {TIMESTAMP}")

    LOG_RUN_DIR = os.path.join(LOG_DIR, f"{TIMESTAMP}_{TRAINING_MODE}")

    if not os.path.exists(CHECKPOINT_DIR) and (args.resume or args.checkpoint_dir):
        print(f"Error: Checkpoint directory {CHECKPOINT_DIR} does not exist!")
        sys.exit(1)

    print(f"Global paths set:")
    print(f"  - TRAINING_MODE: {TRAINING_MODE}")
    print(f"  - TIMESTAMP: {TIMESTAMP}")
    print(f"  - CHECKPOINT_DIR: {CHECKPOINT_DIR}")
    print(f"  - LOG_RUN_DIR: {LOG_RUN_DIR}")


def clear_memory():
    """æ¸…ç†GPUå’ŒCPUå†…å­˜"""
    torch.cuda.empty_cache()
    gc.collect()
    current_mem = torch.cuda.memory_allocated() / 1024 ** 3 if torch.cuda.is_available() else 0
    print(f"GPU memory after cleanup: {current_mem:.2f} GB")


def cleanup_training_objects(*objects):
    """æ¸…ç†è®­ç»ƒç›¸å…³å¯¹è±¡"""
    for obj in objects:
        if obj is not None:
            del obj
    clear_memory()


def load_data_unified_split(train_percent=0.8, val_percent=0.2, seed=None):
    """ç»Ÿä¸€åŠ è½½æ‰€æœ‰æ•°æ®é›†ï¼Œç„¶åéšæœºåˆ’åˆ†ï¼Œå¹¶è®¡ç®—åŠ é€Ÿåº¦æ ‡å‡†åŒ–å‚æ•°"""
    print("Loading unified dataset with consistent split method...")

    if seed is not None:
        torch.manual_seed(seed)
        np.random.seed(seed)

    try:
        all_data_folders = TRAIN_DATA_FOLDERS + VAL_DATA_FOLDERS
        print(f"Combining datasets from {len(all_data_folders)} folders:")
        for folder in all_data_folders:
            print(f"  - {folder}")

        unified_dataset = ImuDataset(all_data_folders)
        total_size = len(unified_dataset)
        print(f"Total unified dataset size: {total_size} samples")

        train_size = int(total_size * train_percent)
        val_size = total_size - train_size

        print(f"Dataset split:")
        print(f"  - Training: {train_size} samples ({train_size / total_size * 100:.1f}%)")
        print(f"  - Validation: {val_size} samples ({val_size / total_size * 100:.1f}%)")

        from torch.utils.data import random_split
        train_dataset, val_dataset = random_split(
            unified_dataset,
            [train_size, val_size],
            generator=torch.Generator().manual_seed(seed) if seed else None
        )

    except Exception as e:
        print(f"Error loading unified dataset: {e}")
        sys.exit(1)

    num_workers = 0 if sys.platform == "win32" else 2

    # åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼ˆç”¨äºè®¡ç®—æ ‡å‡†åŒ–å‚æ•°ï¼‰
    train_loader_for_norm = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,  # è®¡ç®—ç»Ÿè®¡é‡æ—¶ä¸éœ€è¦shuffle
        pin_memory=True,
        num_workers=num_workers
    )

    # è®¡ç®—åŠ é€Ÿåº¦æ ‡å‡†åŒ–å‚æ•°
    print("ğŸ“Š Computing acceleration normalization statistics...")
    normalizer = AccelerationNormalizer()
    if not normalizer.fit(train_loader_for_norm):
        print("âŒ Failed to compute normalization statistics!")
        sys.exit(1)

    # åˆ›å»ºæ­£å¼çš„æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        pin_memory=True,
        num_workers=num_workers
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE_VAL,
        shuffle=False,
        pin_memory=True,
        num_workers=num_workers
    )

    print(f"Data loaders created successfully!")
    print(f"  - Training batches: {len(train_loader)}")
    print(f"  - Validation batches: {len(val_loader)}")

    return train_loader, val_loader, normalizer


def check_data_distribution(train_loader, val_loader, normalizer, num_samples=3):
    """æ£€æŸ¥è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ•°æ®åˆ†å¸ƒä¸€è‡´æ€§ï¼ˆåŒ…æ‹¬æ ‡å‡†åŒ–åçš„åˆ†å¸ƒï¼‰"""
    print("\n=== Data Distribution Analysis ===")

    def compute_stats(data_loader, name, normalizer, max_batches=num_samples):
        stats = {
            'acc_mean': [], 'acc_std': [], 'acc_norm_mean': [], 'acc_norm_std': [],
            'ori_mean': [], 'ori_std': [],
            'pos_mean': [], 'pos_std': []
        }

        count = 0
        with torch.no_grad():
            for data in data_loader:
                if count >= max_batches:
                    break

                try:
                    acc = data[0].float()
                    ori = data[2].float()
                    pos = data[3].float()

                    # è¿‡æ»¤æç«¯å€¼
                    if acc.abs().max() < 100 and ori.abs().max() < 20:
                        # åŸå§‹æ•°æ®ç»Ÿè®¡
                        stats['acc_mean'].append(acc.mean().item())
                        stats['acc_std'].append(acc.std().item())
                        stats['ori_mean'].append(ori.mean().item())
                        stats['ori_std'].append(ori.std().item())
                        stats['pos_mean'].append(pos.mean().item())
                        stats['pos_std'].append(pos.std().item())

                        # æ ‡å‡†åŒ–åçš„åŠ é€Ÿåº¦ç»Ÿè®¡
                        if normalizer and normalizer.fitted:
                            acc_norm = normalizer.normalize_acc(acc)
                            stats['acc_norm_mean'].append(acc_norm.mean().item())
                            stats['acc_norm_std'].append(acc_norm.std().item())

                    count += 1
                except Exception:
                    count += 1
                    continue

        # è®¡ç®—å¹³å‡ç»Ÿè®¡é‡
        for key in stats:
            if stats[key]:
                stats[key] = np.mean(stats[key])
            else:
                stats[key] = 0.0

        return stats

    print("Computing training set statistics...")
    train_stats = compute_stats(train_loader, "Train", normalizer)

    print("Computing validation set statistics...")
    val_stats = compute_stats(val_loader, "Validation", normalizer)

    print(f"\nDistribution Comparison (based on {num_samples} batches):")
    print(f"{'Metric':<20} {'Train':<12} {'Validation':<12} {'Difference':<12}")
    print("-" * 60)

    for key in train_stats:
        train_val = train_stats[key]
        val_val = val_stats[key]
        diff = abs(train_val - val_val)
        print(f"{key:<20} {train_val:<12.6f} {val_val:<12.6f} {diff:<12.6f}")

    # é‡ç‚¹å…³æ³¨æ ‡å‡†åŒ–åçš„åŠ é€Ÿåº¦åˆ†å¸ƒ
    if train_stats['acc_norm_mean'] != 0:
        print(f"\nğŸ” Normalized Acceleration Analysis:")
        print(f"   - Train norm acc mean: {train_stats['acc_norm_mean']:.6f} (should be ~0)")
        print(f"   - Train norm acc std:  {train_stats['acc_norm_std']:.6f} (should be ~1)")
        print(f"   - Val norm acc mean:   {val_stats['acc_norm_mean']:.6f} (should be ~0)")
        print(f"   - Val norm acc std:    {val_stats['acc_norm_std']:.6f} (should be ~1)")

    total_diff = sum([abs(train_stats[key] - val_stats[key]) for key in ['acc_mean', 'ori_mean', 'pos_mean']])
    print(f"\nTotal Difference Score: {total_diff:.6f} (lower is better)")

    if total_diff < 0.1:
        print("âœ“ Data distributions appear consistent!")
    elif total_diff < 0.5:
        print("âš  Data distributions have minor differences")
    else:
        print("âŒ Data distributions have significant differences")

    return train_stats, val_stats


def rotation_matrix_loss_stable(pred_6d, target_6d):
    """æ•°å€¼ç¨³å®šçš„æ—‹è½¬çŸ©é˜µæŸå¤±"""
    try:
        batch_size, seq_len, joints, _ = pred_6d.shape
        pred_6d_flat = pred_6d.reshape(-1, 6)
        target_6d_flat = target_6d.reshape(-1, 6)

        # æ£€æŸ¥è¾“å…¥æœ‰æ•ˆæ€§
        if torch.isnan(pred_6d_flat).any() or torch.isnan(target_6d_flat).any():
            print("rotation is nan")
            return torch.tensor(10.0, device=pred_6d.device, requires_grad=True)

        pred_rotmat = r6d_to_rotation_matrix(pred_6d_flat)
        target_rotmat = r6d_to_rotation_matrix(target_6d_flat)

        # æ£€æŸ¥æ—‹è½¬çŸ©é˜µæœ‰æ•ˆæ€§
        if torch.isnan(pred_rotmat).any() or torch.isnan(target_rotmat).any():
            print("rotation is nan_1")
            return torch.tensor(10.0, device=pred_6d.device, requires_grad=True)

        # ä½¿ç”¨æ›´ç¨³å®šçš„æŸå¤±è®¡ç®—
        diff = pred_rotmat - target_rotmat
        loss = torch.mean(torch.norm(diff, dim=(-2, -1)) + 1e-8)

        # æ·»åŠ æ•°å€¼è£å‰ª
        loss = torch.clamp(loss, 0.0, 100.0)

        # æœ€ç»ˆæ£€æŸ¥
        if torch.isnan(loss) or torch.isinf(loss):
            return torch.tensor(10.0, device=pred_6d.device, requires_grad=True)

        return loss
    except Exception as e:
        print(f"Error in rotation_matrix_loss: {e}")
        return torch.tensor(10.0, device=pred_6d.device, requires_grad=True)


# ===== ç«¯åˆ°ç«¯è”åˆè®­ç»ƒå‡½æ•° =====ï¼ˆåŠç²¾åº¦ï¼‰
# def train_end_to_end_joint(model1, model2, model3, optimizer, scheduler, train_loader, val_loader,
#                            normalizer, epochs, early_stopper, start_epoch=0):
#     """ç«¯åˆ°ç«¯è”åˆè®­ç»ƒæ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹ï¼ˆåªå¯¹åŠ é€Ÿåº¦æ•°æ®æ ‡å‡†åŒ–ï¼‰"""
#     print("\n====================== Starting End-to-End Joint Training =========================")
#     print(f"ğŸš€ Using Joint E2E Training Mode - All models trained simultaneously")
#     print(f"ğŸ“Š Checkpoint saving to: {early_stopper.path}")
#     print(f"ğŸ“ Data normalization: ONLY Acceleration data")
#
#     criterion = nn.MSELoss(reduction='mean')
#     # scaler = GradScaler(device='cuda' if torch.cuda.is_available() else 'cpu')  # ä¿®å¤åºŸå¼ƒè­¦å‘Š
#     writer = SummaryWriter(os.path.join(LOG_RUN_DIR, 'joint_e2e_logs')) if LOG_ENABLED else None
#
#     # è°ƒæ•´æŸå¤±æƒé‡
#     loss_weights = {
#         'leaf_pos': 0.00,
#         'all_pos': 0.00,
#         'pose_6d': 1
#     }
#
#     print(f"ğŸ“ˆ Loss weights: {loss_weights}")
#     print(f"ğŸ“Š Using acceleration normalization: {normalizer.fitted}")
#
#     for epoch in range(start_epoch, epochs):
#         current_epoch = epoch + 1
#
#         model1.train()
#         model2.train()
#         model3.train()
#
#         train_losses = {'total': [], 'leaf_pos': [], 'all_pos': [], 'pose_6d': []}
#         valid_batches = 0
#         skipped_batches = 0
#
#         epoch_pbar = tqdm(train_loader, desc=f"ğŸ”„ Joint E2E Epoch {current_epoch}/{epochs}", leave=True)
#
#         # === è®­ç»ƒå¾ªç¯ ===
#         for batch_idx, data in enumerate(epoch_pbar):
#             try:
#                 acc = data[0].to(DEVICE, non_blocking=True).float()
#                 ori_6d = data[2].to(DEVICE, non_blocking=True).float()  # ä¸æ ‡å‡†åŒ–
#                 p_leaf_gt = data[3].to(DEVICE, non_blocking=True).float()  # ä¸æ ‡å‡†åŒ–
#                 p_all_gt = data[4].to(DEVICE, non_blocking=True).float()  # ä¸æ ‡å‡†åŒ–
#                 pose_6d_gt = data[6].to(DEVICE, non_blocking=True).float()  # ä¸æ ‡å‡†åŒ–
#
#                 # æ•°æ®è´¨é‡æ£€æŸ¥
#                 is_valid, reason = improved_data_check(acc, ori_6d, p_leaf_gt)
#                 if not is_valid:
#                     skipped_batches += 1
#                     if skipped_batches <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªè­¦å‘Š
#                         print(f"Warning: Skipping batch {batch_idx} - {reason}")
#                     continue
#
#                 # ğŸ”¥ åªå¯¹åŠ é€Ÿåº¦æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–
#                 try:
#                     acc_norm = normalizer.normalize_acc(acc)  # åªæ ‡å‡†åŒ–åŠ é€Ÿåº¦
#                     # ori_6d, p_leaf_gt, p_all_gt, pose_6d_gt ä¿æŒåŸæ ·
#                 except Exception as e:
#                     print(f"Error in acceleration normalization: {e}")
#                     skipped_batches += 1
#                     continue
#
#                 optimizer.zero_grad(set_to_none=True)
#
#                 with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):
#                     # === ä½¿ç”¨æ ‡å‡†åŒ–çš„åŠ é€Ÿåº¦æ•°æ®è¿›è¡Œå‰å‘ä¼ æ’­ ===
#                     # FDIP_1: é¢„æµ‹å¶èŠ‚ç‚¹ä½ç½®
#                     input1 = torch.cat((acc_norm, ori_6d), -1).view(acc_norm.shape[0], acc_norm.shape[1], -1)
#                     p_leaf_pred = model1(input1)
#
#                     # FDIP_2: é¢„æµ‹æ‰€æœ‰å…³èŠ‚ä½ç½®
#                     zeros = torch.zeros(p_leaf_pred.shape[0], p_leaf_pred.shape[1], 3, device=DEVICE)
#                     p_leaf_with_root = torch.cat(
#                         [zeros, p_leaf_pred.view(p_leaf_pred.shape[0], p_leaf_pred.shape[1], -1)], dim=2)
#                     input2 = torch.cat(
#                         [acc_norm, ori_6d,  # ä½¿ç”¨æ ‡å‡†åŒ–çš„åŠ é€Ÿåº¦
#                          p_leaf_with_root.view(p_leaf_with_root.shape[0], p_leaf_with_root.shape[1], 6, 3)],
#                         dim=-1).view(acc_norm.shape[0], acc_norm.shape[1], -1)
#                     p_all_pred = model2(input2)
#
#                     # FDIP_3: é¢„æµ‹6Då§¿æ€
#                     input_base = torch.cat((acc_norm, ori_6d), -1).view(acc_norm.shape[0], acc_norm.shape[1], -1)
#                     pose_6d_pred = model3(input_base, p_all_pred)
#
#                     # === è®¡ç®—æŸå¤±ï¼ˆä½¿ç”¨åŸå§‹ç›®æ ‡æ•°æ®ï¼‰ ===
#                     # å¶èŠ‚ç‚¹ä½ç½®æŸå¤± - ä½¿ç”¨åŸå§‹ç›®æ ‡
#                     loss_leaf = criterion(p_leaf_pred, p_leaf_gt.view(-1, p_leaf_gt.shape[1], 15))
#
#                     # æ‰€æœ‰å…³èŠ‚ä½ç½®æŸå¤± - ä½¿ç”¨åŸå§‹ç›®æ ‡
#                     p_all_target = torch.cat([torch.zeros_like(p_all_gt[:, :, 0:1, :]), p_all_gt], dim=2).view(
#                         p_all_gt.shape[0], p_all_gt.shape[1], -1)
#                     loss_all_pos = criterion(p_all_pred, p_all_target)
#
#                     # 6Då§¿æ€æŸå¤± - ä½¿ç”¨åŸå§‹ç›®æ ‡
#                     batch_size, seq_len = pose_6d_pred.shape[:2]
#                     pose_pred_reshaped = pose_6d_pred.view(batch_size, seq_len, 24, 6)
#                     loss_pose = rotation_matrix_loss_stable(pose_pred_reshaped, pose_6d_gt)
#
#                     # åŠ æƒæ€»æŸå¤±
#                     total_loss = (loss_weights['leaf_pos'] * loss_leaf +
#                                   loss_weights['all_pos'] * loss_all_pos +
#                                   loss_weights['pose_6d'] * loss_pose)
#
#                 # æŸå¤±æœ‰æ•ˆæ€§æ£€æŸ¥
#                 if torch.isnan(total_loss) or torch.isinf(total_loss) or total_loss.item() > 1000.0:
#                     print(f"Warning: NaN or inf in total_loss: {total_loss}")
#                     skipped_batches += 1
#                     continue
#
#                 scaler.scale(total_loss).backward()
#
#                 # æ¢¯åº¦è£å‰ª
#                 scaler.unscale_(optimizer)
#                 grad_norm = torch.nn.utils.clip_grad_norm_(
#                     [p for model in [model1, model2, model3] for p in model.parameters()],
#                     max_norm=1.0
#                 )
#
#                 if torch.isnan(grad_norm):
#                     print('Warning: NaN gradient norm.')
#                     skipped_batches += 1
#                     continue
#
#                 scaler.step(optimizer)
#                 scaler.update()
#
#                 # è®°å½•æŸå¤±
#                 train_losses['total'].append(total_loss.item())
#                 train_losses['leaf_pos'].append(loss_leaf.item())
#                 train_losses['all_pos'].append(loss_all_pos.item())
#                 train_losses['pose_6d'].append(loss_pose.item())
#
#                 valid_batches += 1
#
#                 # æ›´æ–°è¿›åº¦æ¡
#                 if valid_batches % 5 == 0:
#                     epoch_pbar.set_postfix({
#                         'total': f"{total_loss.item():.4f}",
#                         'leaf': f"{loss_leaf.item():.4f}",
#                         'pos': f"{loss_all_pos.item():.4f}",
#                         'pose': f"{loss_pose.item():.4f}",
#                         'valid': f"{valid_batches}",
#                         'skip': f"{skipped_batches}"
#                     })
#
#             except Exception as e:
#                 print(f"Error in training batch {batch_idx}: {e}")
#                 skipped_batches += 1
#                 continue
#
#         # æ£€æŸ¥æœ‰æ•ˆbatchæ•°é‡
#         if valid_batches == 0:
#             print(f"âŒ No valid training batches in epoch {current_epoch}!")
#             continue
#
#         print(f"ğŸ“Š Epoch {current_epoch}: Valid batches: {valid_batches}, Skipped: {skipped_batches}")
#
#         # === éªŒè¯é˜¶æ®µ ===
#         model1.eval()
#         model2.eval()
#         model3.eval()
#         val_losses = {'total': [], 'leaf_pos': [], 'all_pos': [], 'pose_6d': []}
#         valid_val_batches = 0
#
#         with torch.no_grad():
#             for data_val in val_loader:
#                 try:
#                     acc_val = data_val[0].to(DEVICE, non_blocking=True).float()
#                     ori_val = data_val[2].to(DEVICE, non_blocking=True).float()
#                     p_leaf_gt_val = data_val[3].to(DEVICE, non_blocking=True).float()
#                     p_all_gt_val = data_val[4].to(DEVICE, non_blocking=True).float()
#                     pose_6d_gt_val = data_val[6].to(DEVICE, non_blocking=True).float()
#
#                     # éªŒè¯æ•°æ®è´¨é‡æ£€æŸ¥
#                     is_valid, _ = improved_data_check(acc_val, ori_val, p_leaf_gt_val)
#                     if not is_valid:
#                         print("val is_valid")
#                         continue
#
#                     # éªŒè¯æ•°æ®æ ‡å‡†åŒ– - åªå¯¹åŠ é€Ÿåº¦
#                     acc_val_norm = normalizer.normalize_acc(acc_val)
#
#                     # éªŒè¯å‰å‘ä¼ æ’­
#                     input1_val = torch.cat((acc_val_norm, ori_val), -1).view(acc_val_norm.shape[0], acc_val_norm.shape[1],
#                                                                              -1)
#                     p_leaf_pred_val = model1(input1_val)
#
#                     zeros_val = torch.zeros(p_leaf_pred_val.shape[0], p_leaf_pred_val.shape[1], 3, device=DEVICE)
#                     p_leaf_with_root_val = torch.cat(
#                         [zeros_val, p_leaf_pred_val.view(p_leaf_pred_val.shape[0], p_leaf_pred_val.shape[1], -1)], dim=2)
#                     input2_val = torch.cat(
#                         [acc_val_norm, ori_val,
#                          p_leaf_with_root_val.view(p_leaf_with_root_val.shape[0], p_leaf_with_root_val.shape[1], 6, 3)],
#                         dim=-1).view(acc_val_norm.shape[0], acc_val_norm.shape[1], -1)
#                     p_all_pred_val = model2(input2_val)
#
#                     input_base_val = torch.cat((acc_val_norm, ori_val), -1).view(acc_val_norm.shape[0],
#                                                                                  acc_val_norm.shape[1], -1)
#                     pose_6d_pred_val = model3(input_base_val, p_all_pred_val)
#
#                     # éªŒè¯æŸå¤±è®¡ç®— - ä½¿ç”¨åŸå§‹ç›®æ ‡
#                     loss_leaf_val = criterion(p_leaf_pred_val, p_leaf_gt_val.view(-1, p_leaf_gt_val.shape[1], 15))
#
#                     p_all_target_val = torch.cat([torch.zeros_like(p_all_gt_val[:, :, 0:1, :]), p_all_gt_val], dim=2).view(
#                         p_all_gt_val.shape[0], p_all_gt_val.shape[1], -1)
#                     loss_all_pos_val = criterion(p_all_pred_val, p_all_target_val)
#
#                     batch_size_val, seq_len_val = pose_6d_pred_val.shape[:2]
#                     pose_pred_reshaped_val = pose_6d_pred_val.view(batch_size_val, seq_len_val, 24, 6)
#                     loss_pose_val = rotation_matrix_loss_stable(pose_pred_reshaped_val, pose_6d_gt_val)
#
#                     total_loss_val = (loss_weights['leaf_pos'] * loss_leaf_val +
#                                       loss_weights['all_pos'] * loss_all_pos_val +
#                                       loss_weights['pose_6d'] * loss_pose_val)
#
#                     if not torch.isnan(total_loss_val) and not torch.isinf(
#                             total_loss_val) and total_loss_val.item() < 1000.0:
#                         val_losses['total'].append(total_loss_val.item())
#                         val_losses['leaf_pos'].append(loss_leaf_val.item())
#                         val_losses['all_pos'].append(loss_all_pos_val.item())
#                         val_losses['pose_6d'].append(loss_pose_val.item())
#                         valid_val_batches += 1
#
#                 except Exception as e:
#                     continue
#
#         print(f"Valid validation batches: {valid_val_batches}/{len(val_loader)}")
#
#         # è®¡ç®—å¹³å‡æŸå¤±
#         avg_train_losses = {k: np.mean(v) if v else float('inf') for k, v in train_losses.items()}
#         avg_val_losses = {k: np.mean(v) if v else float('inf') for k, v in val_losses.items()}
#         current_lr = optimizer.param_groups[0]['lr']
#
#         # æ‰“å°è®­ç»ƒç»“æœ
#         print(f'\nğŸ”„ Joint E2E Epoch {current_epoch}/{epochs} | LR: {current_lr:.6f}')
#         print(f'  ğŸ“Š Valid batches: Train={valid_batches}, Val={valid_val_batches}')
#         print(
#             f'  ğŸ“ˆ Train - Total: {avg_train_losses["total"]:.6f}, Leaf: {avg_train_losses["leaf_pos"]:.6f}, Pos: {avg_train_losses["all_pos"]:.6f}, Pose: {avg_train_losses["pose_6d"]:.6f}')
#         print(
#             f'  ğŸ“‰ Val   - Total: {avg_val_losses["total"]:.6f}, Leaf: {avg_val_losses["leaf_pos"]:.6f}, Pos: {avg_val_losses["all_pos"]:.6f}, Pose: {avg_val_losses["pose_6d"]:.6f}')
#
#         # è®¡ç®—æŸå¤±æ¯”ç‡
#         loss_ratio = 0.0
#         if avg_train_losses["total"] > 0 and avg_val_losses["total"] < float('inf'):
#             loss_ratio = avg_val_losses["total"] / avg_train_losses["total"]
#             print(f'  ğŸ“Š Loss Ratio (Val/Train): {loss_ratio:.3f}')
#
#         # è®°å½•åˆ°TensorBoard
#         if LOG_ENABLED and writer:
#             for loss_type in train_losses.keys():
#                 if avg_train_losses[loss_type] < float('inf') and avg_val_losses[loss_type] < float('inf'):
#                     writer.add_scalars(f'joint_e2e_loss/{loss_type}', {
#                         'train': avg_train_losses[loss_type],
#                         'val': avg_val_losses[loss_type]
#                     }, current_epoch)
#             writer.add_scalar('joint_e2e_learning_rate', current_lr, current_epoch)
#             writer.add_scalar('joint_e2e_loss_ratio', loss_ratio, current_epoch)
#             writer.add_scalar('joint_e2e_valid_batches', valid_batches, current_epoch)
#
#         # å­¦ä¹ ç‡è°ƒåº¦
#         scheduler.step(avg_val_losses['total'])
#
#         # æ—©åœæ£€æŸ¥
#         if avg_val_losses['total'] < float('inf') and valid_val_batches > 0:
#             early_stopper(avg_val_losses['total'], [model1, model2, model3], optimizer, current_epoch, normalizer)
#             if early_stopper.early_stop:
#                 print(f"ğŸ›‘ Early stopping triggered at epoch {current_epoch} for Joint E2E Training.")
#                 break
#         else:
#             print(f"âš ï¸ No valid validation batches in epoch {current_epoch}, skipping early stopping check")
#
#         # å†…å­˜æ¸…ç†
#         torch.cuda.empty_cache()
#
#         # è®­ç»ƒå®Œæˆï¼ŒåŠ è½½æœ€ä½³æ¨¡å‹
#     if os.path.exists(early_stopper.path):
#         print(f"âœ… Loading best joint E2E model from epoch {early_stopper.best_epoch}")
#         checkpoint = torch.load(early_stopper.path, map_location=DEVICE, weights_only=False)
#         model1.load_state_dict(checkpoint['model1_state_dict'])
#         model2.load_state_dict(checkpoint['model2_state_dict'])
#         model3.load_state_dict(checkpoint['model3_state_dict'])
#         print(f"âœ… Successfully loaded best models with validation loss: {early_stopper.val_loss_min:.6f}")
#         del checkpoint
#
#         # æ¸…ç†èµ„æº
#     if writer:
#         writer.close()
#     del criterion, scaler
#     torch.cuda.empty_cache()
#
#     print("======================== End-to-End Joint Training Finished =======================================")
#     return model1, model2, model3

def train_end_to_end_joint(model1, model2, model3, optimizer, scheduler, train_loader, val_loader,
                           normalizer, epochs, early_stopper, start_epoch=0):
    """ç«¯åˆ°ç«¯è”åˆè®­ç»ƒæ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹ï¼ˆå…¨ç²¾åº¦è®­ç»ƒï¼Œåªå¯¹åŠ é€Ÿåº¦æ•°æ®æ ‡å‡†åŒ–ï¼‰"""
    print("\n====================== Starting End-to-End Joint Training (Full Precision) =========================")
    print(f"ğŸš€ Using Joint E2E Training Mode - All models trained simultaneously")
    print(f"ğŸ“Š Checkpoint saving to: {early_stopper.path}")
    print(f"ğŸ“ Data normalization: ONLY Acceleration data")
    print(f"âš¡ Training mode: Full Precision (FP32)")

    criterion = nn.MSELoss(reduction='mean')
    # ç§»é™¤ GradScaler - ä½¿ç”¨å…¨ç²¾åº¦è®­ç»ƒ
    writer = SummaryWriter(os.path.join(LOG_RUN_DIR, 'joint_e2e_logs')) if LOG_ENABLED else None

    # è°ƒæ•´æŸå¤±æƒé‡
    loss_weights = {
        'leaf_pos': 0.00,
        'all_pos': 0.00,
        'pose_6d': 1
    }

    print(f"ğŸ“ˆ Loss weights: {loss_weights}")
    print(f"ğŸ“Š Using acceleration normalization: {normalizer.fitted}")

    for epoch in range(start_epoch, epochs):
        current_epoch = epoch + 1

        model1.train()
        model2.train()
        model3.train()

        train_losses = {'total': [], 'leaf_pos': [], 'all_pos': [], 'pose_6d': []}
        valid_batches = 0
        skipped_batches = 0

        epoch_pbar = tqdm(train_loader, desc=f"ğŸ”„ Joint E2E Epoch {current_epoch}/{epochs} (FP32)", leave=True)

        # === è®­ç»ƒå¾ªç¯ ===
        for batch_idx, data in enumerate(epoch_pbar):
            try:
                acc = data[0].to(DEVICE, non_blocking=True).float()
                ori_6d = data[2].to(DEVICE, non_blocking=True).float()  # ä¸æ ‡å‡†åŒ–
                p_leaf_gt = data[3].to(DEVICE, non_blocking=True).float()  # ä¸æ ‡å‡†åŒ–
                p_all_gt = data[4].to(DEVICE, non_blocking=True).float()  # ä¸æ ‡å‡†åŒ–
                pose_6d_gt = data[6].to(DEVICE, non_blocking=True).float()  # ä¸æ ‡å‡†åŒ–

                # æ•°æ®è´¨é‡æ£€æŸ¥
                is_valid, reason = improved_data_check(acc, ori_6d, p_leaf_gt)
                if not is_valid:
                    skipped_batches += 1
                    if skipped_batches <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªè­¦å‘Š
                        print(f"Warning: Skipping batch {batch_idx} - {reason}")
                    continue

                # ğŸ”¥ åªå¯¹åŠ é€Ÿåº¦æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–
                try:
                    acc_norm = normalizer.normalize_acc(acc)  # åªæ ‡å‡†åŒ–åŠ é€Ÿåº¦
                    # ori_6d, p_leaf_gt, p_all_gt, pose_6d_gt ä¿æŒåŸæ ·
                except Exception as e:
                    print(f"Error in acceleration normalization: {e}")
                    skipped_batches += 1
                    continue

                optimizer.zero_grad(set_to_none=True)

                # === ç§»é™¤ autocastï¼Œä½¿ç”¨å…¨ç²¾åº¦å‰å‘ä¼ æ’­ ===
                # FDIP_1: é¢„æµ‹å¶èŠ‚ç‚¹ä½ç½®
                input1 = torch.cat((acc_norm, ori_6d), -1).view(acc_norm.shape[0], acc_norm.shape[1], -1)
                p_leaf_pred = model1(input1)

                # FDIP_2: é¢„æµ‹æ‰€æœ‰å…³èŠ‚ä½ç½®
                zeros = torch.zeros(p_leaf_pred.shape[0], p_leaf_pred.shape[1], 3, device=DEVICE)
                p_leaf_with_root = torch.cat(
                    [zeros, p_leaf_pred.view(p_leaf_pred.shape[0], p_leaf_pred.shape[1], -1)], dim=2)
                input2 = torch.cat(
                    [acc_norm, ori_6d,  # ä½¿ç”¨æ ‡å‡†åŒ–çš„åŠ é€Ÿåº¦
                     p_leaf_with_root.view(p_leaf_with_root.shape[0], p_leaf_with_root.shape[1], 6, 3)],
                    dim=-1).view(acc_norm.shape[0], acc_norm.shape[1], -1)
                p_all_pred = model2(input2)

                # FDIP_3: é¢„æµ‹6Då§¿æ€
                input_base = torch.cat((acc_norm, ori_6d), -1).view(acc_norm.shape[0], acc_norm.shape[1], -1)
                pose_6d_pred = model3(input_base, p_all_pred)

                # === è®¡ç®—æŸå¤±ï¼ˆä½¿ç”¨åŸå§‹ç›®æ ‡æ•°æ®ï¼‰ ===
                # å¶èŠ‚ç‚¹ä½ç½®æŸå¤± - ä½¿ç”¨åŸå§‹ç›®æ ‡
                loss_leaf = criterion(p_leaf_pred, p_leaf_gt.view(-1, p_leaf_gt.shape[1], 15))

                # æ‰€æœ‰å…³èŠ‚ä½ç½®æŸå¤± - ä½¿ç”¨åŸå§‹ç›®æ ‡
                p_all_target = torch.cat([torch.zeros_like(p_all_gt[:, :, 0:1, :]), p_all_gt], dim=2).view(
                    p_all_gt.shape[0], p_all_gt.shape[1], -1)
                loss_all_pos = criterion(p_all_pred, p_all_target)

                # 6Då§¿æ€æŸå¤± - ä½¿ç”¨åŸå§‹ç›®æ ‡
                batch_size, seq_len = pose_6d_pred.shape[:2]
                pose_pred_reshaped = pose_6d_pred.view(batch_size, seq_len, 24, 6)
                loss_pose = rotation_matrix_loss_stable(pose_pred_reshaped, pose_6d_gt)

                # åŠ æƒæ€»æŸå¤±
                total_loss = (loss_weights['leaf_pos'] * loss_leaf +
                              loss_weights['all_pos'] * loss_all_pos +
                              loss_weights['pose_6d'] * loss_pose)

                # æŸå¤±æœ‰æ•ˆæ€§æ£€æŸ¥
                if torch.isnan(total_loss) or torch.isinf(total_loss) or total_loss.item() > 1000.0:
                    print(f"Warning: NaN or inf in total_loss: {total_loss}")
                    skipped_batches += 1
                    continue

                # === å…¨ç²¾åº¦åå‘ä¼ æ’­ - ç§»é™¤scaler ===
                total_loss.backward()

                # æ¢¯åº¦è£å‰ª
                grad_norm = torch.nn.utils.clip_grad_norm_(
                    [p for model in [model1, model2, model3] for p in model.parameters()],
                    max_norm=100.0
                )

                if torch.isnan(grad_norm):
                    print('Warning: NaN gradient norm.')
                    skipped_batches += 1
                    continue

                # === å…¨ç²¾åº¦ä¼˜åŒ–å™¨æ›´æ–° - ç§»é™¤scaler ===
                optimizer.step()

                # è®°å½•æŸå¤±
                train_losses['total'].append(total_loss.item())
                train_losses['leaf_pos'].append(loss_leaf.item())
                train_losses['all_pos'].append(loss_all_pos.item())
                train_losses['pose_6d'].append(loss_pose.item())

                valid_batches += 1

                # æ›´æ–°è¿›åº¦æ¡
                if valid_batches % 5 == 0:
                    epoch_pbar.set_postfix({
                        'total': f"{total_loss.item():.4f}",
                        'leaf': f"{loss_leaf.item():.4f}",
                        'pos': f"{loss_all_pos.item():.4f}",
                        'pose': f"{loss_pose.item():.4f}",
                        'valid': f"{valid_batches}",
                        'skip': f"{skipped_batches}",
                        'mode': 'FP32'
                    })

            except Exception as e:
                print(f"Error in training batch {batch_idx}: {e}")
                skipped_batches += 1
                continue

        # æ£€æŸ¥æœ‰æ•ˆbatchæ•°é‡
        if valid_batches == 0:
            print(f"âŒ No valid training batches in epoch {current_epoch}!")
            continue

        print(f"ğŸ“Š Epoch {current_epoch} (FP32): Valid batches: {valid_batches}, Skipped: {skipped_batches}")

        # === éªŒè¯é˜¶æ®µ ===
        model1.eval()
        model2.eval()
        model3.eval()
        val_losses = {'total': [], 'leaf_pos': [], 'all_pos': [], 'pose_6d': []}
        valid_val_batches = 0

        with torch.no_grad():
            for data_val in val_loader:
                try:
                    acc_val = data_val[0].to(DEVICE, non_blocking=True).float()
                    ori_val = data_val[2].to(DEVICE, non_blocking=True).float()
                    p_leaf_gt_val = data_val[3].to(DEVICE, non_blocking=True).float()
                    p_all_gt_val = data_val[4].to(DEVICE, non_blocking=True).float()
                    pose_6d_gt_val = data_val[6].to(DEVICE, non_blocking=True).float()

                    # éªŒè¯æ•°æ®è´¨é‡æ£€æŸ¥
                    is_valid, _ = improved_data_check(acc_val, ori_val, p_leaf_gt_val)
                    if not is_valid:
                        print("val is_valid")
                        continue

                    # éªŒè¯æ•°æ®æ ‡å‡†åŒ– - åªå¯¹åŠ é€Ÿåº¦
                    acc_val_norm = normalizer.normalize_acc(acc_val)

                    # === å…¨ç²¾åº¦éªŒè¯å‰å‘ä¼ æ’­ ===
                    input1_val = torch.cat((acc_val_norm, ori_val), -1).view(acc_val_norm.shape[0], acc_val_norm.shape[1], -1)
                    p_leaf_pred_val = model1(input1_val)

                    zeros_val = torch.zeros(p_leaf_pred_val.shape[0], p_leaf_pred_val.shape[1], 3, device=DEVICE)
                    p_leaf_with_root_val = torch.cat(
                        [zeros_val, p_leaf_pred_val.view(p_leaf_pred_val.shape[0], p_leaf_pred_val.shape[1], -1)], dim=2)
                    input2_val = torch.cat(
                        [acc_val_norm, ori_val,
                         p_leaf_with_root_val.view(p_leaf_with_root_val.shape[0], p_leaf_with_root_val.shape[1], 6, 3)],
                        dim=-1).view(acc_val_norm.shape[0], acc_val_norm.shape[1], -1)
                    p_all_pred_val = model2(input2_val)

                    input_base_val = torch.cat((acc_val_norm, ori_val), -1).view(acc_val_norm.shape[0],
                                                                                 acc_val_norm.shape[1], -1)
                    pose_6d_pred_val = model3(input_base_val, p_all_pred_val)

                    # éªŒè¯æŸå¤±è®¡ç®— - ä½¿ç”¨åŸå§‹ç›®æ ‡
                    loss_leaf_val = criterion(p_leaf_pred_val, p_leaf_gt_val.view(-1, p_leaf_gt_val.shape[1], 15))

                    p_all_target_val = torch.cat([torch.zeros_like(p_all_gt_val[:, :, 0:1, :]), p_all_gt_val], dim=2).view(
                        p_all_gt_val.shape[0], p_all_gt_val.shape[1], -1)
                    loss_all_pos_val = criterion(p_all_pred_val, p_all_target_val)

                    batch_size_val, seq_len_val = pose_6d_pred_val.shape[:2]
                    pose_pred_reshaped_val = pose_6d_pred_val.view(batch_size_val, seq_len_val, 24, 6)
                    loss_pose_val = rotation_matrix_loss_stable(pose_pred_reshaped_val, pose_6d_gt_val)

                    total_loss_val = (loss_weights['leaf_pos'] * loss_leaf_val +
                                      loss_weights['all_pos'] * loss_all_pos_val +
                                      loss_weights['pose_6d'] * loss_pose_val)

                    if not torch.isnan(total_loss_val) and not torch.isinf(
                            total_loss_val) and total_loss_val.item() < 1000.0:
                        val_losses['total'].append(total_loss_val.item())
                        val_losses['leaf_pos'].append(loss_leaf_val.item())
                        val_losses['all_pos'].append(loss_all_pos_val.item())
                        val_losses['pose_6d'].append(loss_pose_val.item())
                        valid_val_batches += 1

                except Exception as e:
                    continue

        print(f"Valid validation batches: {valid_val_batches}/{len(val_loader)}")

        # è®¡ç®—å¹³å‡æŸå¤±
        avg_train_losses = {k: np.mean(v) if v else float('inf') for k, v in train_losses.items()}
        avg_val_losses = {k: np.mean(v) if v else float('inf') for k, v in val_losses.items()}
        current_lr = optimizer.param_groups[0]['lr']

        # æ‰“å°è®­ç»ƒç»“æœ
        print(f'\nğŸ”„ Joint E2E Epoch {current_epoch}/{epochs} (FP32) | LR: {current_lr:.6f}')
        print(f'  ğŸ“Š Valid batches: Train={valid_batches}, Val={valid_val_batches}')
        print(
            f'  ğŸ“ˆ Train - Total: {avg_train_losses["total"]:.6f}, Leaf: {avg_train_losses["leaf_pos"]:.6f}, Pos: {avg_train_losses["all_pos"]:.6f}, Pose: {avg_train_losses["pose_6d"]:.6f}')
        print(
            f'  ğŸ“‰ Val   - Total: {avg_val_losses["total"]:.6f}, Leaf: {avg_val_losses["leaf_pos"]:.6f}, Pos: {avg_val_losses["all_pos"]:.6f}, Pose: {avg_val_losses["pose_6d"]:.6f}')

        # è®¡ç®—æŸå¤±æ¯”ç‡
        loss_ratio = 0.0
        if avg_train_losses["total"] > 0 and avg_val_losses["total"] < float('inf'):
            loss_ratio = avg_val_losses["total"] / avg_train_losses["total"]
            print(f'  ğŸ“Š Loss Ratio (Val/Train): {loss_ratio:.3f}')

        # è®°å½•åˆ°TensorBoard
        if LOG_ENABLED and writer:
            for loss_type in train_losses.keys():
                if avg_train_losses[loss_type] < float('inf') and avg_val_losses[loss_type] < float('inf'):
                    writer.add_scalars(f'joint_e2e_loss/{loss_type}', {
                        'train': avg_train_losses[loss_type],
                        'val': avg_val_losses[loss_type]
                    }, current_epoch)
            writer.add_scalar('joint_e2e_learning_rate', current_lr, current_epoch)
            writer.add_scalar('joint_e2e_loss_ratio', loss_ratio, current_epoch)
            writer.add_scalar('joint_e2e_valid_batches', valid_batches, current_epoch)

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(avg_val_losses['total'])

        # æ—©åœæ£€æŸ¥
        if avg_val_losses['total'] < float('inf') and valid_val_batches > 0:
            early_stopper(avg_val_losses['total'], [model1, model2, model3], optimizer, current_epoch, normalizer)
            if early_stopper.early_stop:
                print(f"ğŸ›‘ Early stopping triggered at epoch {current_epoch} for Joint E2E Training (FP32).")
                break
        else:
            print(f"âš ï¸ No valid validation batches in epoch {current_epoch}, skipping early stopping check")

        # å†…å­˜æ¸…ç†
        torch.cuda.empty_cache()

    # è®­ç»ƒå®Œæˆï¼ŒåŠ è½½æœ€ä½³æ¨¡å‹
    if os.path.exists(early_stopper.path):
        print(f"âœ… Loading best joint E2E model (FP32) from epoch {early_stopper.best_epoch}")
        checkpoint = torch.load(early_stopper.path, map_location=DEVICE, weights_only=False)
        model1.load_state_dict(checkpoint['model1_state_dict'])
        model2.load_state_dict(checkpoint['model2_state_dict'])
        model3.load_state_dict(checkpoint['model3_state_dict'])
        print(f"âœ… Successfully loaded best models with validation loss: {early_stopper.val_loss_min:.6f}")
        del checkpoint

    # æ¸…ç†èµ„æº
    if writer:
        writer.close()
    del criterion
    torch.cuda.empty_cache()

    print("======================== End-to-End Joint Training (Full Precision) Finished =======================================")
    return model1, model2, model3





# ===== è¯„ä¼°å‡½æ•° =====
def evaluate_pipeline(model1, model2, model3, data_loader, normalizer):
    """è¯„ä¼°æµæ°´çº¿ï¼ˆä½¿ç”¨åŠ é€Ÿåº¦æ ‡å‡†åŒ–ï¼‰"""
    print("\n============================ Evaluating Complete Pipeline ======================================")
    print(f"ğŸ“Š Evaluation for {TRAINING_MODE} training mode with acceleration normalization")

    clear_memory()

    # è¯„ä¼°ç›®å½•
    eval_results_dir = os.path.join("GGIP", f"evaluate_{TRAINING_MODE}_pipeline_{TIMESTAMP}")
    eval_plots_dir = os.path.join(eval_results_dir, "plots")
    eval_data_dir = os.path.join(eval_results_dir, "data")

    # åˆ›å»ºç›®å½•
    os.makedirs(eval_results_dir, exist_ok=True)
    os.makedirs(eval_plots_dir, exist_ok=True)
    os.makedirs(eval_data_dir, exist_ok=True)

    try:
        # æ£€æŸ¥SMPLæ¨¡å‹æ–‡ä»¶
        smpl_path = r"F:\FDIP\basicmodel_m_lbs_10_207_0_v1.0.0.pkl"
        if not os.path.exists(smpl_path):
            print(f"âŒ SMPL model file not found: {smpl_path}")
            print("ğŸ”§ Please download SMPL model and place it at the correct path")

            # è¿è¡Œç®€åŒ–è¯„ä¼°
            print("ğŸ”„ Running simplified evaluation without SMPL...")
            simplified_evaluation(model1, model2, model3, data_loader, normalizer, eval_data_dir)
            return

        evaluator = PerFramePoseEvaluator()
        model1.eval()
        model2.eval()
        model3.eval()

        all_errors = {
            "pos_err": [],
            "mesh_err": [],
            "angle_err": [],
            "jitter_err": []
        }

        print(f"Running {TRAINING_MODE} model evaluation with acceleration normalization...")
        valid_eval_batches = 0

        with torch.no_grad():
            for data_val in tqdm(data_loader, desc=f"Evaluating {TRAINING_MODE.upper()} Pipeline"):
                try:
                    acc, ori_6d, pose_6d_gt = [d.to(DEVICE, non_blocking=True).float() for d in
                                               (data_val[0], data_val[2], data_val[6])]

                    # æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥
                    is_valid, _ = improved_data_check(acc, ori_6d)
                    if not is_valid:
                        print("evaluate is_valid")
                        continue

                    # ğŸ”¥ åªå¯¹åŠ é€Ÿåº¦è¿›è¡Œæ ‡å‡†åŒ–
                    acc_norm = normalizer.normalize_acc(acc)

                    # çº§è”æ¨ç†
                    input1 = torch.cat((acc_norm, ori_6d), -1).view(acc_norm.shape[0], acc_norm.shape[1], -1)
                    p_leaf_logits = model1(input1)

                    zeros1 = torch.zeros(p_leaf_logits.shape[0], p_leaf_logits.shape[1], 3, device=DEVICE)
                    p_leaf_pred = torch.cat(
                        [zeros1, p_leaf_logits.view(p_leaf_logits.shape[0], p_leaf_logits.shape[1], -1)], dim=2)

                    input2 = torch.cat(
                        [acc_norm, ori_6d, p_leaf_pred.view(p_leaf_pred.shape[0], p_leaf_pred.shape[1], 6, 3)],
                        dim=-1).view(acc_norm.shape[0], acc_norm.shape[1], -1)
                    p_all_pos_flattened = model2(input2)

                    input_base = torch.cat((acc_norm, ori_6d), -1).view(acc_norm.shape[0], acc_norm.shape[1], -1)
                    pose_pred_flat = model3(input_base, p_all_pos_flattened)

                    batch_size, seq_len = pose_pred_flat.shape[:2]
                    pose_pred = pose_pred_flat.view(batch_size, seq_len, 24, 6)

                    errs_dict = evaluator.eval(pose_pred, pose_6d_gt)

                    for key in all_errors.keys():
                        if key in errs_dict and errs_dict[key].numel() > 0:
                            all_errors[key].append(errs_dict[key].flatten().cpu())

                    valid_eval_batches += 1

                except Exception as e:
                    print(f"Warning: Error processing batch in evaluation: {e}")
                    continue

        print(f"Processed {valid_eval_batches} valid evaluation batches")

        # è¯„ä¼°ç»“æœå¤„ç†
        clear_memory()

        if all_errors["mesh_err"] and valid_eval_batches > 0:
            print("Processing evaluation results...")

            # æ‹¼æ¥æ‰€æœ‰è¯¯å·®æ•°æ®
            final_errors = {key: torch.cat(val, dim=0) for key, val in all_errors.items() if val}
            avg_errors = {key: val.mean().item() for key, val in final_errors.items()}

            # æ‰“å°ç»“æœ
            print(f"\nğŸ¯ {TRAINING_MODE.upper()} Pipeline Evaluation Results (Mean):")
            print(f"  - Positional Error (cm):      {avg_errors.get('pos_err', 'N/A'):.4f}")
            print(f"  - Mesh Error (cm):            {avg_errors.get('mesh_err', 'N/A'):.4f}")
            print(f"  - Angular Error (deg):        {avg_errors.get('angle_err', 'N/A'):.4f}")
            print(f"  - Jitter Error (cm/sÂ²):       {avg_errors.get('jitter_err', 'N/A'):.4f}")

            # ä¿å­˜è¯„ä¼°ç»“æœ
            save_evaluation_results(final_errors, avg_errors, eval_data_dir)

        else:
            print("âŒ No evaluation results were generated or no valid batches processed.")

    except Exception as e:
        print(f"Critical error in evaluation pipeline: {e}")
        print("ğŸ”„ Attempting simplified evaluation...")
        simplified_evaluation(model1, model2, model3, data_loader, normalizer, eval_data_dir)

    print(f"\nâœ… {TRAINING_MODE.upper()} evaluation completed. Results saved in: {eval_results_dir}")


def simplified_evaluation(model1, model2, model3, data_loader, normalizer, eval_data_dir):
    """ç®€åŒ–çš„è¯„ä¼°å‡½æ•°ï¼Œä¸ä¾èµ–SMPLæ¨¡å‹"""
    print("Running simplified evaluation (MSE-based metrics)...")

    model1.eval()
    model2.eval()
    model3.eval()

    mse_losses = []
    valid_batches = 0

    with torch.no_grad():
        for data_val in tqdm(data_loader, desc="Simplified Evaluation"):
            try:
                acc, ori_6d, pose_6d_gt = [d.to(DEVICE, non_blocking=True).float() for d in
                                           (data_val[0], data_val[2], data_val[6])]

                # æ•°æ®æ£€æŸ¥
                is_valid, _ = improved_data_check(acc, ori_6d)
                if not is_valid:
                    print("simple_evaluate is_valid")
                    continue

                # åŠ é€Ÿåº¦æ ‡å‡†åŒ–
                acc_norm = normalizer.normalize_acc(acc)

                # æ¨¡å‹æ¨ç†
                input1 = torch.cat((acc_norm, ori_6d), -1).view(acc_norm.shape[0], acc_norm.shape[1], -1)
                p_leaf_logits = model1(input1)

                zeros1 = torch.zeros(p_leaf_logits.shape[0], p_leaf_logits.shape[1], 3, device=DEVICE)
                p_leaf_pred = torch.cat(
                    [zeros1, p_leaf_logits.view(p_leaf_logits.shape[0], p_leaf_logits.shape[1], -1)], dim=2)

                input2 = torch.cat(
                    [acc_norm, ori_6d, p_leaf_pred.view(p_leaf_pred.shape[0], p_leaf_pred.shape[1], 6, 3)],
                    dim=-1).view(acc_norm.shape[0], acc_norm.shape[1], -1)
                p_all_pos_flattened = model2(input2)

                input_base = torch.cat((acc_norm, ori_6d), -1).view(acc_norm.shape[0], acc_norm.shape[1], -1)
                pose_pred_flat = model3(input_base, p_all_pos_flattened)

                # è®¡ç®—MSEæŸå¤±
                mse_loss = nn.MSELoss()(pose_pred_flat, pose_6d_gt.view(pose_6d_gt.shape[0], pose_6d_gt.shape[1], -1))
                mse_losses.append(mse_loss.item())
                valid_batches += 1

            except Exception as e:
                continue

    if mse_losses:
        avg_mse = np.mean(mse_losses)
        std_mse = np.std(mse_losses)

        print(f"\nğŸ¯ Simplified Evaluation Results (Acceleration Normalized):")
        print(f"  - Average MSE Loss:           {avg_mse:.6f}")
        print(f"  - Standard Deviation:         {std_mse:.6f}")
        print(f"  - Valid Batches Processed:    {valid_batches}")

        # ä¿å­˜ç®€åŒ–è¯„ä¼°ç»“æœ
        results = {
            'avg_mse': avg_mse,
            'std_mse': std_mse,
            'valid_batches': valid_batches,
            'total_samples': len(mse_losses),
            'normalization_type': 'acceleration_only'
        }

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_path = os.path.join(eval_data_dir, f"simplified_eval_acc_norm_{timestamp}.json")
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"  - Results saved to: {results_path}")
    else:
        print("âŒ No valid evaluation results generated in simplified evaluation")


def save_evaluation_results(final_errors, avg_errors, eval_data_dir):
    """ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    try:
        # 1. ä¿å­˜åŸå§‹è¯¯å·®æ•°æ®
        raw_data_path = os.path.join(eval_data_dir, f"{TRAINING_MODE}_raw_errors_acc_norm_{timestamp}.pkl")
        with open(raw_data_path, 'wb') as f:
            pickle.dump(final_errors, f)
        print(f"Raw error data saved to: {raw_data_path}")

        # 2. ä¿å­˜ç»Ÿè®¡ç»“æœ
        stats_data = {
            "timestamp": timestamp,
            "training_mode": TRAINING_MODE,
            "normalization_type": "acceleration_only",
            "evaluation_results": {
                "mean_errors": avg_errors,
                "sample_counts": {key: len(val) for key, val in final_errors.items()},
                "std_errors": {key: val.std().item() for key, val in final_errors.items()},
                "min_errors": {key: val.min().item() for key, val in final_errors.items()},
                "max_errors": {key: val.max().item() for key, val in final_errors.items()}
            },
            "units": {
                "pos_err": "cm",
                "mesh_err": "cm",
                "angle_err": "degrees",
                "jitter_err": "cm/sÂ²"
            }
        }

        stats_path = os.path.join(eval_data_dir, f"{TRAINING_MODE}_evaluation_stats_acc_norm_{timestamp}.json")
        with open(stats_path, 'w') as f:
            json.dump(stats_data, f, indent=2)
        print(f"Statistics saved to: {stats_path}")

        # 3. ä¿å­˜ä¸ºCSVæ ¼å¼
        csv_data = []
        for key, values in final_errors.items():
            for value in values.numpy():
                csv_data.append({
                    'training_mode': TRAINING_MODE,
                    'normalization_type': 'acceleration_only',
                    'metric': key,
                    'value': value,
                    'timestamp': timestamp
                })

        if csv_data:
            df = pd.DataFrame(csv_data)
            csv_path = os.path.join(eval_data_dir, f"{TRAINING_MODE}_evaluation_data_acc_norm_{timestamp}.csv")
            df.to_csv(csv_path, index=False)
            print(f"CSV data saved to: {csv_path}")

    except Exception as e:
        print(f"Warning: Error saving data files: {e}")


# ===== ä¸»è®­ç»ƒå‡½æ•° =====
def main():
    """æ”¹è¿›çš„ä¸»å‡½æ•°ï¼Œæ”¯æŒåŠ é€Ÿåº¦æ•°æ®æ ‡å‡†åŒ–çš„ç«¯åˆ°ç«¯è”åˆè®­ç»ƒ"""
    args = parse_args()

    global BATCH_SIZE, LEARNING_RATE, MAX_EPOCHS, PATIENCE
    BATCH_SIZE = args.batch_size
    LEARNING_RATE = args.learning_rate
    MAX_EPOCHS = args.max_epochs
    PATIENCE = args.patience

    set_seed(SEED)
    print("==================== Starting Improved Training Pipeline =====================")
    print(f"Training mode: {'Joint End-to-End' if args.use_joint_training else 'Sequential Stages'}")
    print(f"Residual connections: {'Enabled' if args.use_residual else 'Disabled'}")
    print(f"Data normalization: ONLY Acceleration data")
    print(f"Reset best on resume: {'Enabled' if args.reset_best_on_resume else 'Disabled'}")

    setup_directories_and_paths(args)
    create_directories()

    # æ•°æ®åŠ è½½
    try:
        train_loader, val_loader, normalizer = load_data_unified_split(
            train_percent=0.8,
            val_percent=0.2,
            seed=SEED
        )
        print("Using unified dataset with consistent split and acceleration normalization!")
        check_data_distribution(train_loader, val_loader, normalizer)

        # ä¿å­˜æ ‡å‡†åŒ–å‚æ•°
        norm_path = os.path.join(CHECKPOINT_DIR, 'acceleration_normalizer.pth')
        normalizer.save(norm_path)
        print(f"Acceleration normalization parameters saved to: {norm_path}")

    except Exception as e:
        print(f"Failed to load unified datasets: {e}")
        sys.exit(1)

    total_start_time = time.time()

    if args.use_joint_training:
        print(f"\n=== Using End-to-End Joint Training Mode ===")

        # åˆå§‹åŒ–æ¨¡å‹
        model1 = FDIP_1(input_dim=6 * 9, output_dim=5 * 3).to(DEVICE)

        if args.use_residual:
            model2 = FDIP_2_Residual(input_dim=6 * 12, output_dim=24 * 3).to(DEVICE)
            model3 = FDIP_3_Residual(input_dim=6 * 9, output_dim=24 * 6).to(DEVICE)
            print("Using residual-enhanced models")
        else:
            model2 = FDIP_2(input_dim=6 * 12, output_dim=24 * 3).to(DEVICE)
            model3 = FDIP_3(input_dim=6 * 9, output_dim=24 * 6).to(DEVICE)
            print("Using original models")

        # æ£€æŸ¥ç‚¹è·¯å¾„
        checkpoint_path = os.path.join(CHECKPOINT_DIR, 'joint_e2e_training', 'best_joint_e2e_model.pth')
        completion_marker = os.path.join(CHECKPOINT_DIR, 'joint_e2e_training', 'joint_e2e_completed.marker')

        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆè®­ç»ƒ
        if os.path.exists(completion_marker) and not args.resume:
            print("Joint E2E training already completed. Loading best models and skipping training.")
            if os.path.exists(checkpoint_path):
                checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)
                model1.load_state_dict(checkpoint['model1_state_dict'])
                model2.load_state_dict(checkpoint['model2_state_dict'])
                model3.load_state_dict(checkpoint['model3_state_dict'])
                print(f"Successfully loaded best joint E2E models from {checkpoint_path}")
                del checkpoint

                # åŠ è½½æ ‡å‡†åŒ–å™¨
                norm_checkpoint_path = checkpoint_path.replace('.pth', '_normalizer.pth')
                if os.path.exists(norm_checkpoint_path):
                    normalizer.load(norm_checkpoint_path)
            else:
                print(f"Error: Completion marker found, but checkpoint file {checkpoint_path} is missing!")
                sys.exit(1)
        else:
            # è®¾ç½®è”åˆä¼˜åŒ–å™¨ - æ”¹è¿›çš„å­¦ä¹ ç‡ç­–ç•¥
            param_groups = [
                {'params': model1.parameters(), 'lr': LEARNING_RATE, 'weight_decay': WEIGHT_DECAY},
                {'params': model2.parameters(), 'lr': LEARNING_RATE * 0.7, 'weight_decay': WEIGHT_DECAY},
                {'params': model3.parameters(), 'lr': LEARNING_RATE * 0.1, 'weight_decay': WEIGHT_DECAY},  # æœ€å°å­¦ä¹ ç‡
            ]
            optimizer = optim.AdamW(param_groups)

            scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode='min',
                factor=0.7,
                patience=8,
                min_lr=1e-6,
                threshold=1e-4
            )

            # åˆ›å»ºæ—©åœå™¨ï¼Œä¼ å…¥é‡ç½®æ ‡å¿—
            early_stopper = MultiModelEarlyStopping(
                patience=PATIENCE,
                path=checkpoint_path,
                verbose=True,
                reset_best=args.reset_best_on_resume  # ä¼ å…¥é‡ç½®æ ‡å¿—
            )

            start_epoch = 0
            is_resuming = False

            # æ£€æŸ¥æ˜¯å¦æœ‰æ–­ç‚¹å¯ä»¥æ¢å¤
            if os.path.exists(checkpoint_path):
                print(f"Found joint E2E checkpoint. Resuming training from: {checkpoint_path}")
                checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)
                model1.load_state_dict(checkpoint['model1_state_dict'])
                model2.load_state_dict(checkpoint['model2_state_dict'])
                model3.load_state_dict(checkpoint['model3_state_dict'])
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                start_epoch = checkpoint['epoch']

                # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡ç½®æœ€ä½³è®°å½•
                if args.reset_best_on_resume:
                    print("ğŸ”„ Reset best validation loss requested for resume training")
                    early_stopper.reset_best_values(start_epoch)
                else:
                    # åŠ è½½ä¹‹å‰çš„æ—©åœçŠ¶æ€
                    early_stopper.val_loss_min = checkpoint['val_loss_min']
                    early_stopper.best_score = checkpoint['best_score']
                    early_stopper.counter = checkpoint.get('early_stopping_counter', 0)
                    print(f"ğŸ“Š Loaded previous best validation loss: {early_stopper.val_loss_min:.6f}")

                print(f"Resuming from Epoch {start_epoch + 1}")
                is_resuming = True

                # åŠ è½½æ ‡å‡†åŒ–å™¨
                norm_checkpoint_path = checkpoint_path.replace('.pth', '_normalizer.pth')
                if os.path.exists(norm_checkpoint_path):
                    normalizer.load(norm_checkpoint_path)

                del checkpoint
            else:
                print("No joint E2E checkpoint found. Starting training from scratch.")

            # å¦‚æœæ˜¯ç»­è®­ä¸”è®¾ç½®äº†é‡ç½®æ ‡å¿—ï¼Œç»™å‡ºé¢å¤–æç¤º
            if is_resuming and args.reset_best_on_resume:
                print("âš ï¸  IMPORTANT: Best validation loss has been reset!")
                print("   The first validation result will be saved as the new best checkpoint.")
                print("   This is useful when loss weights or training configuration have changed.")

            # æ‰§è¡Œè”åˆè®­ç»ƒ
            model1, model2, model3 = train_end_to_end_joint(
                model1, model2, model3, optimizer, scheduler,
                train_loader, val_loader, normalizer,
                MAX_EPOCHS, early_stopper, start_epoch
            )

            # è®­ç»ƒå®Œæˆæ ‡è®°
            with open(completion_marker, 'w') as f:
                f.write(f"Joint E2E training completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Training mode: {TRAINING_MODE}\n")
                f.write(f"Residual connections: {'Enabled' if args.use_residual else 'Disabled'}\n")
                f.write(f"Normalization: Acceleration data only\n")
                f.write(f"Reset best on resume: {'Enabled' if args.reset_best_on_resume else 'Disabled'}\n")
                f.write(
                    f"Best model saved at epoch {early_stopper.best_epoch} with val_loss {early_stopper.val_loss_min:.6f}\n")
            print(f"Joint E2E training marked as completed.")

            # æ¸…ç†è®­ç»ƒå¯¹è±¡
            cleanup_training_objects(optimizer, scheduler, early_stopper)

    else:
        print("\nUsing Sequential Stage Training Mode")
        print("Sequential training not implemented in this version. Please use joint training.")
        return

    print(f"\nAll {TRAINING_MODE} training complete!")
    total_end_time = time.time()
    print(f"Total training time: {(total_end_time - total_start_time) / 3600:.2f} hours")

    clear_memory()
    evaluate_pipeline(model1, model2, model3, val_loader, normalizer)
    print(f"\nImproved {TRAINING_MODE} training and evaluation finished successfully!")

if __name__ == '__main__':
    main()



